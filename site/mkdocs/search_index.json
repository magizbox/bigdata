{
    "docs": [
        {
            "location": "/", 
            "text": "Big Data Q\nA\n\n\n1. What is \"Big Data\"? \n1\n\n\nhttps://www.youtube.com/watch?v=TzxmjbL-i4Y\n\n\n2. How big is big data? \n2\n\n\n\n\n3. How much data is \"Big Data\"? \n3\n\n\n\n\n4. What are characteristics of \"Big Data\"? \n4\n\n\n\n\n5. What is big data ecosystem? \n5\n\n\n\n\n6. What is big data landscape \n6\n\n\n\n\n7. What are benefits of big data? \n7\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://www.youtube.com/watch?v=TzxmjbL-i4Y\n\n\n\n\n\n\nhttp://scoop.intel.com/what-happens-in-an-internet-minute/\n\n\n\n\n\n\nhttp://www.quora.com/How-much-data-is-Big-Data\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Big_data#Characteristics\n\n\n\n\n\n\nhttp://www.clearpeaks.com/blog/big-data/big-data-ecosystem-spark-and-tableau\n\n\n\n\n\n\nhttps://vladimerbotsvadze.wordpress.com/2015/01/28/the-big-data-landscape-technology-businessintelligence-analytics/\n\n\n\n\n\n\nhttp://blog.galaxyweblinks.com/big-data-with-bigger-benefits/", 
            "title": "Home"
        }, 
        {
            "location": "/#big-data-qa", 
            "text": "", 
            "title": "Big Data Q&amp;A"
        }, 
        {
            "location": "/#1-what-is-big-data-1", 
            "text": "https://www.youtube.com/watch?v=TzxmjbL-i4Y", 
            "title": "1. What is \"Big Data\"? 1"
        }, 
        {
            "location": "/#2-how-big-is-big-data-2", 
            "text": "", 
            "title": "2. How big is big data? 2"
        }, 
        {
            "location": "/#3-how-much-data-is-big-data-3", 
            "text": "", 
            "title": "3. How much data is \"Big Data\"? 3"
        }, 
        {
            "location": "/#4-what-are-characteristics-of-big-data-4", 
            "text": "", 
            "title": "4. What are characteristics of \"Big Data\"? 4"
        }, 
        {
            "location": "/#5-what-is-big-data-ecosystem-5", 
            "text": "", 
            "title": "5. What is big data ecosystem? 5"
        }, 
        {
            "location": "/#6-what-is-big-data-landscape-6", 
            "text": "", 
            "title": "6. What is big data landscape 6"
        }, 
        {
            "location": "/#7-what-are-benefits-of-big-data-7", 
            "text": "https://www.youtube.com/watch?v=TzxmjbL-i4Y    http://scoop.intel.com/what-happens-in-an-internet-minute/    http://www.quora.com/How-much-data-is-Big-Data    https://en.wikipedia.org/wiki/Big_data#Characteristics    http://www.clearpeaks.com/blog/big-data/big-data-ecosystem-spark-and-tableau    https://vladimerbotsvadze.wordpress.com/2015/01/28/the-big-data-landscape-technology-businessintelligence-analytics/    http://blog.galaxyweblinks.com/big-data-with-bigger-benefits/", 
            "title": "7. What are benefits of big data? 7"
        }, 
        {
            "location": "/hdfs/", 
            "text": "HDFS\n\n\n\n\n\n  The Hadoop Distributed File System (HDFS) \u2014 a subproject of the Apache Hadoop project\u2014is a distributed, highly fault-tolerant file system designed to run on low-cost commodity hardware. HDFS provides high-throughput access to application data and is suitable for applications with large data sets. This article explores the primary features of HDFS and provides a high-level view of the HDFS architecture.\n\n\n\n\n: \nsequenceiq/hadoop-docker\n\n\nBig Data Stack\n: \nHDFS\n, \nKibana\n, \nElasticSearch\n, \nNeo4J\n, \nApache Spark", 
            "title": "HDFS"
        }, 
        {
            "location": "/hdfs/#hdfs", 
            "text": "The Hadoop Distributed File System (HDFS) \u2014 a subproject of the Apache Hadoop project\u2014is a distributed, highly fault-tolerant file system designed to run on low-cost commodity hardware. HDFS provides high-throughput access to application data and is suitable for applications with large data sets. This article explores the primary features of HDFS and provides a high-level view of the HDFS architecture.  :  sequenceiq/hadoop-docker  Big Data Stack :  HDFS ,  Kibana ,  ElasticSearch ,  Neo4J ,  Apache Spark", 
            "title": "HDFS"
        }, 
        {
            "location": "/hbase/", 
            "text": "HBase\n\n\n\n  Apache HBase\u2122 is the Hadoop database, a distributed, scalable, big data store.  Download Apache HBase\u2122  Click here to download Apache HBase\u2122.\n\n\n\n\n\n\n1. When Would I Use Apache HBase? \n1\n\n\n\nHBase isn\u2019t suitable for every problem.\n\n\nFirst, make sure you have enough data. If you have hundreds of millions or billions of rows, then HBase is a good candidate. If you only have a few thousand/million rows, then using a traditional RDBMS might be a better choice due to the fact that all of your data might wind up on a single node (or two) and the rest of the cluster may be sitting idle.\n\n\nSecond, make sure you can live without all the extra features that an RDBMS provides (e.g., typed columns, secondary indexes, transactions, advanced query languages, etc.) An application built against an RDBMS cannot be \"ported\" to HBase by simply changing a JDBC driver, for example. Consider moving from an RDBMS to HBase as a complete redesign as opposed to a port.\n\n\nThird, make sure you have enough hardware. Even HDFS doesn\u2019t do well with anything less than 5 DataNodes (due to things such as HDFS block replication which has a default of 3), plus a NameNode.\n\n\nHBase can run quite well stand-alone on a laptop - but this should be considered a development configuration only.\n\n\n2. Features \n2\n\n\n\n\n\nLinear and modular scalability.\n\n\nStrictly consistent reads and writes.\n\n\nAutomatic and configurable sharding of tables\n\n\nAutomatic failover support between RegionServers.\n\n\nConvenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.\n\n\nEasy to use Java API for client access.\n\n\nBlock cache and Bloom Filters for real-time queries.\n\n\nQuery predicate push down via server side Filters\n\n\nThrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options\n\n\nExtensible jruby-based (JIRB) shell\n\n\nSupport for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX\n\n\n\n\n\n3. Architecture\n\n\n\n\n\nHBase Shell\n\n\n\n[code lang=\"shell\"]\n\n\nlist all table\n\n\nlist\n[/code]\n\n\nUp \n Running\n\n\n\n1. Download \n\n\n\nHBase 0.94.27 (HBase 0.98 won't work)\n\n\n[code lang=\"shell\"]\nwget https://www.apache.org/dist/hbase/hbase-0.94.27/hbase-0.94.27.tar.gz\ntar -xzf hbase-0.94.27.tar.gz\n[/code]\n\n\n2. Setup \n\n\n\n1.\n edit \n$HBASE_ROOT/conf/hbase-site.xml\n and add\n\n\n[code lang=\"xml\"]\n\n\n  \n\n    \nhbase.rootdir\n\n    \nfile:///full/path/to/where/the/data/should/be/stored\n\n  \n\n  \n\n    \nhbase.cluster.distributed\n\n    \nfalse\n\n  \n\n\n\n[/code]\n\n\n3. Verify \n\n\n\nGo to \nhttp://localhost:60010\n to see if HBase is running.\n\n\n\n\n\n\n\n\n\n\n\nWhen Should I Use HBase?\n\n\n\n\n\n\n\nHBase\n\n\n\n\n\n\n\n\n\n\nConfig HBase Remote\n\n\n1. Change \n/etc/hosts\n\n\n[code]\n127.0.0.1 [username]\n[server_ip] hbase.io\n[/code]\n\n\nExample\n\n\n[code]\n127.0.0.1 crawler\n192.168.0.151 hbase.io\n[/code]\n\n\n2. Change hostname\n\n\n[code]\nhostname hbase.io\n[/code]\n\n\n3. Change region servers\n\n\nEdit \n$HBASE_ROOT/conf/regionservers\n\n\n[code]\nhbase.io\n[/code]\n\n\n4. Change \n$HABSE_ROOT/conf/hbase-site.xml\n\n\n[code lang=\"xml\" title=\"hbase-site.xml\"]\n\n?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\n\n\n\n\n\n\nhbase.rootdir\n\n\nfile:///home/username/Downloads/hbase/data\n\n\n\n\n\n\nhbase.cluster.distributed\n\n\nfalse\n\n\n\n\n\n\nhbase.zookeeper.quorum\n\n\nhbase.io\n\n\n\n\n\n\nzookeeper.znode.parent\n\n\n/hbase-unsecure\n\n\n\n\n\n\nhbase.rpc.timeout\n\n\n2592000000\n\n\n\n\n\n[/code]\n\n\nDocker\n\n\nHBase 0.94\n\n\nImage: https://github.com/Banno/docker-hbase-standalone\n\n\n[code]\ndocker run -d -p 2181:2181 -p 60000:60000 -p 60010:60010 -p 60020:60020 -p 60030:60030 banno/hbase-standalone\n[/code]\n\n\nCompose\n\n\n[code]\nhbase.vmware:\n    build: ./docker-hbase-standalone/.\n    command: \"/opt/hbase/hbase-0.94.15-cdh4.7.0/bin/hbase master start\"\n    hostname: hbase.vmware\n    ports:\n      - 2181:2181\n      - 60000:60000\n      - 60010:60010\n      - 60020:60020\n      - 60030:60030\n    volumes:\n      - ./docker-hbase-standalone/hbase-0.94.15-cdh4.7.0:/opt/hbase/hbase-0.94.15-cdh4.7.0\n      - ./data/hbase:/tmp/hbase-root/hbase\n/code]", 
            "title": "Apache HBase"
        }, 
        {
            "location": "/hbase/#hbase", 
            "text": "Apache HBase\u2122 is the Hadoop database, a distributed, scalable, big data store.  Download Apache HBase\u2122  Click here to download Apache HBase\u2122.", 
            "title": "HBase"
        }, 
        {
            "location": "/hbase/#list-all-table", 
            "text": "list\n[/code]", 
            "title": "list all table"
        }, 
        {
            "location": "/hbase/#config-hbase-remote", 
            "text": "", 
            "title": "Config HBase Remote"
        }, 
        {
            "location": "/hbase/#1-change-etchosts", 
            "text": "[code]\n127.0.0.1 [username]\n[server_ip] hbase.io\n[/code]  Example  [code]\n127.0.0.1 crawler\n192.168.0.151 hbase.io\n[/code]", 
            "title": "1. Change /etc/hosts"
        }, 
        {
            "location": "/hbase/#2-change-hostname", 
            "text": "[code]\nhostname hbase.io\n[/code]", 
            "title": "2. Change hostname"
        }, 
        {
            "location": "/hbase/#3-change-region-servers", 
            "text": "Edit  $HBASE_ROOT/conf/regionservers  [code]\nhbase.io\n[/code]", 
            "title": "3. Change region servers"
        }, 
        {
            "location": "/hbase/#4-change-habse_rootconfhbase-sitexml", 
            "text": "[code lang=\"xml\" title=\"hbase-site.xml\"] ?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?    hbase.rootdir  file:///home/username/Downloads/hbase/data    hbase.cluster.distributed  false    hbase.zookeeper.quorum  hbase.io    zookeeper.znode.parent  /hbase-unsecure    hbase.rpc.timeout  2592000000   \n[/code]", 
            "title": "4. Change $HABSE_ROOT/conf/hbase-site.xml"
        }, 
        {
            "location": "/hbase/#docker", 
            "text": "HBase 0.94  Image: https://github.com/Banno/docker-hbase-standalone  [code]\ndocker run -d -p 2181:2181 -p 60000:60000 -p 60010:60010 -p 60020:60020 -p 60030:60030 banno/hbase-standalone\n[/code]  Compose  [code]\nhbase.vmware:\n    build: ./docker-hbase-standalone/.\n    command: \"/opt/hbase/hbase-0.94.15-cdh4.7.0/bin/hbase master start\"\n    hostname: hbase.vmware\n    ports:\n      - 2181:2181\n      - 60000:60000\n      - 60010:60010\n      - 60020:60020\n      - 60030:60030\n    volumes:\n      - ./docker-hbase-standalone/hbase-0.94.15-cdh4.7.0:/opt/hbase/hbase-0.94.15-cdh4.7.0\n      - ./data/hbase:/tmp/hbase-root/hbase\n/code]", 
            "title": "Docker"
        }, 
        {
            "location": "/spark/", 
            "text": "Apache Spark\n\n\n\n\n\n  Apache Spark is an open-source \ncluster computing framework\n originally developed in the AMPLab at UC Berkeley. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's in-memory primitives provide performance up to 100 times faster for certain applications. By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms.\n\n\n\n\nInstallation\n\n\n\nRequirements: Hadoop, YARN\n\n\nInstall Hadoop\n\n\nInsatll YARN\n\n\nInstall Java\n\n\nVerification\n\n\n\nTutorial\n\n\n\nFrom Pandas to Apache Spark\u2019s DataFrame\n\n\nBig Data Stack\n: \nHDFS\n, \nKibana\n, \nElasticSearch\n, \nNeo4J\n, \nApache Spark\n\n\nApache Spark: Tutorials\n\n\nBeginners Guide: Apache Spark Machine Learning with Large Data\n\n\n\n\nSpark and Spark Streaming Unit Testing\n\n\nRecipes for Running Spark Streaming Applications in Production- Databricks\n\n\nSpark Streaming\n\n\n\n\nSpark and Spark Streaming Unit Testing\n\n\nRecipes for Running Spark Streaming Applications in Production- Databricks", 
            "title": "Apache Spark"
        }, 
        {
            "location": "/spark/#apache-spark", 
            "text": "Apache Spark is an open-source  cluster computing framework  originally developed in the AMPLab at UC Berkeley. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's in-memory primitives provide performance up to 100 times faster for certain applications. By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms.", 
            "title": "Apache Spark"
        }, 
        {
            "location": "/spark/#apache-spark-tutorials", 
            "text": "Beginners Guide: Apache Spark Machine Learning with Large Data", 
            "title": "Apache Spark: Tutorials"
        }, 
        {
            "location": "/spark/#spark-streaming", 
            "text": "Spark and Spark Streaming Unit Testing  Recipes for Running Spark Streaming Applications in Production- Databricks", 
            "title": "Spark Streaming"
        }, 
        {
            "location": "/ambari/", 
            "text": "Ambari\n\n\nThe Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.\n\n\nAmbari enables System Administrators to:\n\n\n\n\n\n\nProvision a Hadoop Cluster\n\n\n\n\nAmbari provides a step-by-step wizard for installing Hadoop services across any number of hosts.\n\n\nAmbari handles configuration of Hadoop services for the cluster.\n\n\n\n\n\n\n\n\nManage a Hadoop Cluster\n\n\n\n\nAmbari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.\n\n\n\n\n\n\n\n\nMonitor a Hadoop Cluster\n\n\n\n\nAmbari provides a dashboard for monitoring health and status of the Hadoop cluster.\n\n\nAmbari leverages Ambari Metrics System for metrics collection.\n\n\nAmbari leverages Ambari Alert Framework for system alerting and will notify you when your attention is needed (e.g., a node goes down, remaining disk space is low, etc).\n\n\n\n\n\n\n\n\nAmbari enables Application Developers and System Integrators to:\n\n\n\n\nEasily integrate Hadoop provisioning, management, and monitoring capabilities to their own applications with the Ambari REST APIs.\n\n\n\n\nDocker\n\n\n\n\nReceipts:\n\n\n\n\nImage: \nsequenceiq/ambari\n (\ngit\n)\n\n\n\n\nMultinode cluster with Ambari 1.7.0 \n1\n\n\nGet the docker images\n\n\n[code]\ndocker pull sequenceiq/ambari:1.7.0\n[/code]\n\n\nGet ambari-functions\n[code]\ncurl -Lo .amb j.mp/docker-ambari-170 \n . .amb\n[/code]\n\n\nCreate your cluster \u2013 automated\n\n\n[code]\namb-deploy-cluster 3\n[/code]\n\n\n\n\n\n\n\n\n\n\nMultinode cluster with Ambari 1.7.0", 
            "title": "Apache Ambari"
        }, 
        {
            "location": "/ambari/#ambari", 
            "text": "The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.  Ambari enables System Administrators to:    Provision a Hadoop Cluster   Ambari provides a step-by-step wizard for installing Hadoop services across any number of hosts.  Ambari handles configuration of Hadoop services for the cluster.     Manage a Hadoop Cluster   Ambari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.     Monitor a Hadoop Cluster   Ambari provides a dashboard for monitoring health and status of the Hadoop cluster.  Ambari leverages Ambari Metrics System for metrics collection.  Ambari leverages Ambari Alert Framework for system alerting and will notify you when your attention is needed (e.g., a node goes down, remaining disk space is low, etc).     Ambari enables Application Developers and System Integrators to:   Easily integrate Hadoop provisioning, management, and monitoring capabilities to their own applications with the Ambari REST APIs.", 
            "title": "Ambari"
        }, 
        {
            "location": "/ambari/#docker", 
            "text": "Receipts:   Image:  sequenceiq/ambari  ( git )", 
            "title": "Docker"
        }, 
        {
            "location": "/ambari/#multinode-cluster-with-ambari-170-1", 
            "text": "Get the docker images  [code]\ndocker pull sequenceiq/ambari:1.7.0\n[/code]  Get ambari-functions\n[code]\ncurl -Lo .amb j.mp/docker-ambari-170   . .amb\n[/code]  Create your cluster \u2013 automated  [code]\namb-deploy-cluster 3\n[/code]      Multinode cluster with Ambari 1.7.0", 
            "title": "Multinode cluster with Ambari 1.7.0 1"
        }, 
        {
            "location": "/nutch/", 
            "text": "Nutch\n\n\n\n  \nHighly extensible, highly scalable Web crawler \n1\n Nutch is a well matured, production ready Web crawler. Nutch 1.x enables fine grained configuration, relying on Apache Hadoop\u2122 data structures, which are great for batch processing.\n\n\n\n\n\nHistory\n\n\n\n\n\nUsecases\n\n\n\n\n\n\n1\n Features \n1\n \n\n\n\n1\n Transparency\n Nutch is open source, so anyone can see how the ranking algorithms work. With commercial search engines, the precise details of the algorithms are secret so you can never know why a particular search result is ranked as it is. Furthermore, some search engines allow rankings to be based on payments, rather than on the relevance of the site's contents. Nutch is a good fit for academic and government organizations, where the perception of fairness of rankings may be more important.\n\n\n\n2\n Understanding\n We don't have the source code to Google, so Nutch is probably the best we have. It's interesting to see how a large search engine works. Nutch has been built using ideas from academia and industry: for instance, core parts of Nutch are currently being re-implemented to use the \nMapReduce\n.\n\n\n\nMap Reduce distributed processing model, which emerged from Google Labs last year. And Nutch is attractive for researchers who want to try out new search algorithms, since it is so easy to extend.\n\n\n\n3\n Extensibility\n Don't like the way other search engines display their results? Write your own search engine--using Nutch! Nutch is very flexible: it can be customized and incorporated into your application. For developers, Nutch is a great platform for adding search to heterogeneous collections of information, and being able to customize the search interface, or extend the out-of-the-box functionality through the plugin mechanism. For example, you can integrate it into your site to add a search capability.\n\n\n\n2\n Architectures \n2\n \n3\n\n\n\n\n\n\nData Structures \n4\n\n\n\nThe web database\n is a specialized persistent data structure for mirroring the structure and properties of the web graph being crawled. It persists as long as the web graph that is being crawled (and re-crawled) exists, which may be months or years. The WebDB is used only by the crawler and does not play any role during searching. The WebDB stores two types of entities: pages and links.\n\n\n\nA page\n represents a page on the Web, and is indexed by its URL and the MD5 hash of its contents. Other pertinent information is stored, too, including\n\n\n\n\n\nthe number of links in the page (also called outlinks);\n\n\nfetch information (such as when the page is due to be refetched);\n\n\nthe page's score, which is a measure of how important the page is (for example, one measure of importance awards high scores to pages that are linked to from many other pages).\n\n\n\n\n\nA link\n represents a link from one web page (the source) to another (the target). In the WebDB web graph, the nodes are pages and the edges are links.\n\n\n\nA segment\n is a collection of pages fetched and indexed by the crawler in a single run. The fetchlist for a segment is a list of URLs for the crawler to fetch, and is generated from the WebDB. The fetcher output is the data retrieved from the pages in the fetchlist. The fetcher output for the segment is indexed and the index is stored in the segment. Any given segment has a limited lifespan, since it is obsolete as soon as all of its pages have been re-crawled. The default re-fetch interval is 30 days, so it is usually a good idea to delete segments older than this, particularly as they take up so much disk space. Segments are named by the date and time they were created, so it's easy to tell how old they are.\n\n\n\nThe index\n is the inverted index of all of the pages the system has retrieved, and is created by merging all of the individual segment indexes. Nutch uses Lucene for its indexing, so all of the Lucene tools and APIs are available to interact with the generated index. Since this has the potential to cause confusion, it is worth mentioning that the Lucene index format has a concept of segments, too, and these are different from Nutch segments. A Lucene segment is a portion of a Lucene index, whereas a Nutch segment is a fetched and indexed portion of the WebDB.\n\n\n\nView \ngora-hbase-mapping.xml\n for more details\n\n\n\nProcess \n5\n\n\n\n0\n initialize CrawlDb, inject \nseed\n URLs Repeat \ngenerate-fetch-update\n cycle n times:\n\n\n\n1\n The \nInjector\n takes all the URLs of the nutch.txt file and adds them to the \nCrawlDB\n. As a central part of Nutch, the \nCrawlDB\n maintains information on all known URLs (fetch schedule, fetch status, metadata, \u2026).\n\n\n\n2\n Based on the data of \nCrawlDB\n, the \nGenerator\n creates a fetchlist and places it in a newly created \nSegment directory\n.\n\n\n\n3\n Next, the \nFetcher\n gets the content of the URLs on the fetchlist and writes it back to the \nSegment directory\n. This step usually is the most time-consuming one.\n\n\n\n4\n Now the \nParser\n processes the content of each web page and for example omits all html tags. If the crawl functions as an update or an extension to an already existing one (e.g. depth of 3), the \nUpdater\n would add the new data to the \nCrawlDB\n as a next step.\n\n\n\n5\n Before indexing, all the links need to be inverted by \nLink Inverter\n, which takes into account that not the number of outgoing links of a web page is of interest, but rather the number of inbound links. This is quite similar to how Google PageRank works and is important for the scoring function. The inverted links are saved in the \nLinkdb\n.\n\n\n\n6-7.\n Using data from all possible sources (\nCrawlDB\n, \nLinkDB\n and \nSegments\n), the \nIndexer\n creates an index and saves it within the Solr directory. For indexing, the popular Lucene library is used. Now, the user can search for information regarding the crawled web pages via Solr.\n\n\n\n3\n Install Nutch \n6\n\n\n\nRequirements\n\n\n\nRequire\n\n\n\n1\n OpenJDK 7 \n ant\n\n\n\n2\n Nutch 2.3 RC (yes, you need 2.3, 2.2 will not work)\n\n\n\n[code lang=\"shell\"]\nwget https://archive.apache.org/dist/nutch/2.3/apache-nutch-2.3-src.tar.gz\ntar -xzf apache-nutch-2.3-src.tar.gz\n[/code]\n\n\n3\n HBase 0.94.27 (HBase 0.98 won't work)\n\n\n\n[code lang=\"shell\"]\nwget https://www.apache.org/dist/hbase/hbase-0.94.27/hbase-0.94.27.tar.gz\ntar -xzf hbase-0.94.27.tar.gz\n[/code]\n\n\n4\n ElasticSearch 1.7\n\n\n\n[code lang=\"sh\"]\nwget https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.0.tar.gz\ntar -xzf elasticsearch-1.7.0.tar.gz\n[/code]\n\n\nOther Options: \nnutch-2.3\n, \nhbase-0.94.26\n, \nElasticSearch 1.4\n\n\nSetup HBase\n\n\n\n1\n edit \n$HBASE_ROOT/conf/hbase-site.xml\n and add\n\n\n\n[code lang=\"xml\"]\n\n\n    \n\n        \nhbase.rootdir\n\n        \nfile:///full/path/to/where/the/data/should/be/stored\n\n    \n\n    \n\n        \nhbase.cluster.distributed\n\n        \nfalse\n\n    \n\n\n\n[/code]\n\n\n2\n edit \n$HBASE_ROOT/conf/hbase-env.sh\n and enable \nJAVA_HOME\n and set it to the proper path:\n\n\n\n[code lang=\"diff\"]\n-# export JAVA_HOME=/usr/java/jdk1.6.0/ +export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/\n[/code]\n\n\nThis step might seem redundant, but even with \nJAVA_HOME\n being set in my shell, HBase just didn't recognize it.\n\n\n\n3\n kick off HBase:\n\n\n\n$HBASE_ROOT/bin/start-hbase.sh\n\n\n\n4\n Setting up Nutch\n\n\n\n1\n enable the HBase dependency in\n\n\n\n$NUTCH_ROOT/ivy/ivy.xml\n by uncommenting the line\n\n\n\ndependency org=\"org.apache.gora\" name=\"gora-hbase\" rev=\"0.5\" conf=\"*-\ndefault\" /\n\n\n\n2\n configure the HBase adapter by editing the \n$NUTCH_ROOT/conf/gora.properties\n:\n\n\n\n-#gora.datastore.default=org.apache.gora.mock.store.MockDataStore +gora.datastore.default=org.apache.gora.hbase.store.HBaseStore\n\n\n\n3\n build Nutch\n\n\n\n$ cd $NUTCH_ROOT $ ant clean $ ant runtime\n\n\n\nThis can take a while and creates \n$NUTCH_ROOT/runtime/local\n.\n\n\n\n4\n configure Nutch by editing \n$NUTCH_ROOT/runtime/local/conf/nutch-site.xml\n:\n\n\n\n\n[code lang=\"xml\"]\n\n\n    \n\n        \nhttp.agent.name\n\n        \nmycrawlername\n\n        \n\n    \n\n    \n\n        \nhttp.robots.agents\n\n        \nmycrawlername\n\n        \n\n    \n\n    \n\n        \nstorage.data.store.class\n\n        \norg.apache.gora.hbase.store.HBaseStore\n\n    \n\n    \n\n        \nplugin.includes\n\n        \n\n        \n\n            protocol-httpclient|urlfilter-regex|parse-(text|tika|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-elastic\n        \n\n    \n\n    \n\n        \ndb.ignore.external.links\n\n        \ntrue\n\n        \n\n    \n\n    \n\n        \nelastic.host\n\n        \nlocalhost\n\n        \n\n    \n\n\n\n[/code]\n\nor you configure Nutch by editing \n$NUTCH_ROOT/runtime/local/conf/nutch-site.xml\n:\n\n\n\n[code lang=\"xml\"]\n\n\n    \n\n        \nplugin.includes\n\n        \n\n        \n\n            protocol-http|protocol-httpclient|urlfilter-regex|\nparse-(text|tika|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|\nsummary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-elastic|\nindex-metadata|index-more\n        \n\n    \n\n    \n\n        \ndb.ignore.external.links\n\n        \ntrue\n\n        \n\n    \n\n\n\n\n\n\n  \nelastic.host\n\n  \nlocalhost\n\n  \nThe hostname to send documents to using TransportClient.\n  Either host and port must be defined or cluster.\n  \n\n\n\n\n\n  \nelastic.port\n\n  \n9300\n\n  \n\n  The port to connect to using TransportClient.\n  \n\n\n\n\n\n  \nelastic.index\n\n  \nnutch\n\n  \n\n  The name of the elasticsearch index. Will normally be autocreated if it\n  doesn't exist.\n  \n\n\n\n\n\n\n\n[/code]\n\n\n5\n configure HBase integration by editing \n$NUTCH_ROOT/runtime/local/conf/hbase-site.xml\n:\n\n\n\n[code lang=\"xml\"]\n\n?xml version=\"1.0\" encoding=\"UTF-8\"?\n\n\n\n   \n\n      \nhbase.rootdir\n\n      \nfile:///full/path/to/where/the/data/should/be/stored\n\n      \n\n   \n\n   \n\n      \nhbase.cluster.distributed\n\n      \nfalse\n\n   \n\n\n\n[/code]\nor you configure HBase integration by editing \n$NUTCH_ROOT/runtime/local/conf/hbase-site.xml\n:\n\n[code lang=\"xml\"]\n\n\n  \n\n    \nhbase.rootdir\n\n    \nfile:///$PATH/database\n\n  \n\n  \n\n    \nhbase.cluster.distributed\n\n    \nfalse\n\n  \n\n  \n\n    \nhbase.zookeeper.quorum\n\n    \nhbase.io\n\n  \n\n  \n\n    \nzookeeper.znode.parent\n\n    \n/hbase-unsecure\n\n  \n\n  \n\n    \nhbase.rpc.timeout\n\n    \n2592000000\n\n  \n\n\n\n[/code]\n\n\nThat's it. Everything is now setup to crawl websites.\n\n\n\n5\n Use Nutch\n\n\n\n5\n1 Adding new Domains to crawl with Nutch\n\n\n\n\n\ncreate an empty directory. Add a textfile containing a list of seed URLs. \n\n\n\n\n\n[code lang=\"shell\"]\n$ mkdir seed $ echo \"https://www.website.com\" \n seed/urls.txt\n$ echo \"https://www.another.com\" \n seed/urls.txt\n$ echo \"https://www.example.com\" \n seed/urls.txt\n[/code]\n\n\n\n\ninject them into Nutch by giving a file URL (!) \n\n\n\n\n\n$ $NUTCH_ROOT/runtime/local/bin/nutch inject file:///path/to/seed/ [/code]\n\n\n\n5\n2 Actual Crawling Procedure\n\n\n\n\n\nGenerate a new set of URLs to fetch. This is is based on both the injected URLs as well as outdated URLs in the Nutch crawl db.\n\n\n\n\n\n$ $NUTCH_ROOT/runtime/local/bin/nutch generate -topN 10\n\n\n\nThe above command will create job batches for 10 URLs. 2. Fetch the URLs. We are not clustering, so we can simply fetch all batches:\n\n\n\n\n\n\n$ $NUTCH_ROOT/runtime/local/bin/nutch fetch -all\n\n\n\n\n\nNow we parse all fetched pages:\n\n\n\n\n\n$ $NUTCH_ROOT/runtime/local/bin/nutch parse -all\n\n\n\n\n\nLast step: Update Nutch's internal database: \n\n\n\n\n\n$ $NUTCH_ROOT/runtime/local/bin/nutch updatedb -all\n\n\n\nOn the first run, this will only crawl the injected URLs. The procedure above is supposed to be repeated regulargy to keep the index up to date.\n\n\n\n5\n3 Putting Documents into ElasticSearch Easy peasy:\n\n\n\n$ $NUTCH_ROOT/runtime/local/bin/nutch index -all\n\n\n\n5\n4 Configuration\n\n\n\nCrawl nutch via proxy\n\n\n\nChange \n$NUTCH_ROOT/runtime/local/conf/nutch-site.xml\n\n\n\n[code lang=\"xml\"]\n\n\n    \n\n        \nhttp.proxy.host\n\n        \n192.168.80.1\n\n        \nThe proxy hostname. If empty, no proxy is used.\n\n    \n\n    \n\n        \nhttp.proxy.port\n\n        \nport\n\n        \nThe proxy port.\n\n    \n\n    \n\n        \nhttp.proxy.username\n\n        \nusername\n\n        \nUsername for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic,\n            digest\n            and/or NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of\n            'plugin.includes'\n            property. NOTE: For NTLM authentication, do not prefix the username with the domain, i.e. 'susam' is correct\n            whereas\n            'DOMAINsusam' is incorrect.\n        \n\n    \n\n    \n\n        \nhttp.proxy.password\n\n        \npassword\n\n        \nPassword for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic,\n            digest\n            and/or NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of\n            'plugin.includes'\n            property.\n        \n\n    \n\n\n\n[/code]\n\n\n6\n Nutch Plugins \n7\n\n\n\n6.1 Extension Points\n\n\nIn writing a plugin, you're actually providing one or more extensions of the existing extension-points . The core Nutch extension-points are themselves defined in a plugin, the NutchExtensionPoints plugin (they are listed in the NutchExtensionPoints plugin.xml file). Each extension-point defines an interface that must be implemented by the extension. The core extension points are:\n\n\n\n  \n\n    \n\n      Point\n    \n\n\n    \n\n      Description\n    \n\n\n    \n\n      Example\n    \n\n  \n\n\n  \n\n    \n\n      IndexWriter\n    \n\n\n    \n\n      Writes crawled data to a specific indexing backends (Solr, ElasticSearch, a CVS file, etc.).\n    \n\n\n    \n\n    \n\n  \n\n\n  \n\n    \n\n      IndexingFilter\n    \n\n\n    \n\n      Permits one to add metadata to the indexed fields. All plugins found which implement this extension point are run sequentially on the parse (from javadoc).\n    \n\n\n    \n\n    \n\n  \n\n\n  \n\n    \n\n      Parser\n    \n\n\n    \n\n      Parser implementations read through fetched documents in order to extract data to be indexed. This is what you need to implement if you want Nutch to be able to parse a new type of content, or extract more data from currently parseable content.\n    \n\n\n    \n\n    \n\n  \n\n\n  \n\n    \n\n      HtmlParseFilter\n    \n\n\n    \n\n      Permits one to add additional metadata to HTML parses (from javadoc).\n    \n\n\n    \n\n    \n\n  \n\n\n  \n\n    \n\n      Protocol\n    \n\n\n    \n\n      Protocol implementations allow Nutch to use different protocols (ftp, http, etc.) to fetch documents.\n    \n\n\n    \n\n    \n\n  \n\n\n  \n\n    \n\n      URLFilter\n    \n\n\n    \n\n      URLFilter implementations limit the URLs that Nutch attempts to fetch. The RegexURLFilter distributed with Nutch provides a great deal of control over what URLs Nutch crawls, however if you have very complicated rules about what URLs you want to crawl, you can write your own implementation.\n    \n\n\n    \n\n    \n\n  \n\n\n  \n\n    \n\n      URLNormalizer\n    \n\n\n    \n\n      Interface used to convert URLs to normal form and optionally perform substitutions.\n    \n\n\n    \n\n    \n\n  \n\n\n  \n\n    \n\n      ScoringFilter\n    \n\n\n    \n\n      A contract defining behavior of scoring plugins. A scoring filter will manipulate scoring variables in CrawlDatum and in resulting search indexes. Filters can be chained in a specific order, to provide multi-stage scoring adjustments.\n    \n\n\n    \n\n    \n\n  \n\n\n  \n\n    \n\n      SegmentMergeFilter\n    \n\n\n    \n\n      Interface used to filter segments during segment merge. It allows filtering on more sophisticated criteria than just URLs. In particular it allows filtering based on metadata collected while parsing page.\n    \n\n\n    \n\n    \n\n  \n\n\n\n\n\n6\n2 Getting Nutch to Use a Plugin \n7\n\n\n\nIn order to get Nutch to use a given plugin, you need to edit your conf/nutch-site.xml file and add the name of the plugin to the list of plugin.includes. Additionally we are required to add the various build configurations to build.xml in the plugin directory.\n\n\n\n6.3 Project structure of a plugin\n\n\n[code lang=\"text\"]\nplugin-name\nplugin.xml # file that tells Nutch about the plugin. build.xml # file that tells ant how to build the plugin. ivy.xml # file that describes any dependencies required by the plugin. src org apache nutch indexer uml-meta # source folder URLMetaIndexingFilter.java scoring uml-meta # source folder URLMetaScoringFilter.java test org apache nutch indexer uml-meta # test folder URLMetaIndexingFilterTest.java scoring uml-meta # test folder URLMetaScoringFilterTest.java\n[/code]\n\n\n\n  \nhttp://nutch.apache.org/\n\u00a0\n\u21a9\n\n\n\n\n\n  \nhttps://sites.google.com/site/nutch1936/home/introduction\n\u00a0\n\u21a9\n\n\n\n\n\n  \nWeb Crawling with Apache Nutch\n\u00a0\n\u21a9\n\n\n\n\n\n  \nIntroduction to Nutch, Part 1: Crawling\n\u00a0\n\u21a9\n\n\n\n\n\n  \nNutch \u2013 How It Works\n\u00a0\n\u21a9\n\n\n\n\n\n  \nNutch 2.3 + ElasticSearch 1.7 + HBase 0.94 Setup\n\u00a0\n\u21a9\n\n\n\n\n\n  \nAboutPlugins\n\u00a0\n\u21a9\n \n\u21a9\n \n\n\n\n\n\nConfig nutch run intellij\n\n\nCopy file\n\n\n[code]\n copy all the files in the runtime/conf on out/test/apache-Nutch-2.3 and out/production/apache-Nutch-2.3\n[/code]\n\n\nadd these lines to file \n$NUTCH_SRC/out/test/nutch-site.xml\n\n\n[code]\n\n\n   \nplugin.folders\n\n   \n/build/plugins\n\n \n\n[/code]\n\n\nRun nutch in intellij\n\n\n[code]\nRun-\nEdit Configurations...-\nadd path agrs:path to file list links crawler\n[/code]\n\n\nCrawler\n\n\nConferences\n\n\nWeb Search and Data Mining\n\n\nChallenge\n\n\nStatic Crawler\n\n\n\n\nApache Nutch\n\n\n\n\nDynamic Crawler\n\n\n\n\nnutch-selenium\n\n\n\n\nIntelligent Extractor\n\n\n\n\nboilerpipe\n\n\nWeb Content Extraction Through Machine Learning\n\n\n\n\nPriority Crawler\n\n\nSocial Crawler\n\n\nA New Approach to Dynamic Crawler\n\n\nBuild a crawler system for dynamic websites is not easy task. While you can use a web browser automator (like \nselenium\n), or event when you can integrate selenium with nutch (by using \nnutch-selenium\n). These solutions are still hard to develop, hard to test and hard to manage sessions because we still \"translate\" our process to languages (such as java or python)\n\n\nI suppose a new approach for this problem. Instead of using a  web browser automator, we can inject native javascript codes into browser (via extension or add-on).The advantages of this approach is we can easily inject third party libraries (like \njquery\n (for dom selector), \nRun.js\n (for complicated process) and APIs that supported by browsers). And we can take advance of debugging tool and testing framework in javascript world.\n\n\nIf you want to know about more details, feel free to \ncontact me\n.\n\n\nDev Nutch in Intellij\n\n\nReceipts: \nIntellIJ 14\n, \nApache Nutch 2.3\n\n\n1.\n Get Nutch source\n\n\n[code lang=\"shell\"]\nwget http://www.eu.apache.org/dist/nutch/2.3/apache-nutch-2.3-src.tar.gz\ntar -xzf apache-nutch-2.3-src.tar.gz\n[/code]\n\n\n2.\n Import Nutch source in IntellIJ\n\n\n[wonderplugin_slider id=\"1\"]\n\n\n\n\n\n3.\n Get Dependencies by Ant\n\n\n[wonderplugin_slider id=\"3\"]\n\n\n\n\n\n4.\n Import Dependencies to IntellIJ\n\n\n[wonderplugin_slider id=\"4\"]\n\n\n\n\n\nNutch Dev\n\n\n1.Intasll java in ubuntu\n\n\n-Downloads java version .zip\n[code lang=\"xml\"]\n http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html\n[/code]\n\n\n-Create folder jvm\n[code lang=\"xml\"]\n sudo mkdir /usr/lib/jvm/\n[/code]\n\n\n-Cd to folder downloads java version .zip\n[code lang=\"xml\"]\n sudo mv jdk1.7.0_x/ /usr/lib/jvm/jdk1.7.0_x\n[/code]\n\n\n-Run command line\n[code lang=\"xml\"]\n  sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.7.0_x/jre/bin/java 0\n[/code]\n\n\n-Tets version java\n[code lang=\"xml\"]\n  java -version\n[/code]\n\n\n2.Intasll ant in ubuntu\n\n\n-Downloads ant\n[code lang=\"xml\"]\nhttp://ant.apache.org/manualdownload.cgi\n[/code]\n\n\n-Add path ant vao file environment\n[code lang=\"xml\"]\n sudo nano /etc/environment\n $ANT_ROOT/bin\n[/code]\n\n\n-Run command line\n[code lang=\"xml\"]\nsource /etc/environment\nant -version\n[/code]\n\n\n3.Intasll hbase in ubuntu\n\n\n-Downloads and extract hbase 0.94.27\n[code lang=\"xml\"]\n  https://archive.apache.org/dist/hbase/hbase-0.94.27/\n[/code]\n\n\n-Edit file $HABSE_ROOT/conf/hbase-site.xml\n[code lang=\"xml\"]\n \n\n  \n\n    \nhbase.rootdir\n\n    \nfile:///$PATH_DATA_BASE/database\n\n  \n\n  \n\n    \nhbase.cluster.distributed\n\n    \nfalse\n\n  \n\n  \n\n    \nhbase.zookeeper.quorum\n\n    \nhbase.io\n\n  \n\n  \n\n    \nzookeeper.znode.parent\n\n    \n/hbase-unsecure\n\n  \n\n  \n\n    \nhbase.rpc.timeout\n\n    \n2592000000\n\n  \n\n\n\n[/code]\n\n\n-Edit file $HBASE_ROOT/conf/hbase-env.sh\n[code lang=\"xml\"]\n  export JAVA_HOME=$PATH_JAVA_HOME\n[/code]\n\n\n-Edit file $HBASE_ROOT/conf/regionservers\n[code lang=\"xml\"]\nhbase.io.nutch\n[/code]\n\n\n-Edit file hosts in ubuntu\n[code lang=\"xml\"]\n  sudo nano /etc/hosts\n  {ip} hbase.io.nutch\n[/code]\n\n\n-Edit file hostname in ubuntu\n[code lang=\"xml\"]\n sudo nano /etc/hostname\n hbase.io.nutch\n[/code]\n\n\n-Run and stop hbase in ubuntu\n[code]\n Run hbase : cd $HBASE_ROOT/bin ./start-hbase.sh\n Stop hbase: cd $HBASE_ROOT/bin ./stop-hbase.sh\n[/code]\n\n\n*Error in intasll hbase\n[code lang=\"xml\"]\n- Error regionserver localhost(Edit file hosts and file host name)\n- Error client no remote server intasll hbase(Turn off file firewall)\n[/code]\n\n\n4.Build nutch in ant\n\n\n-Downloads and extract nutch\n[code lang=\"xml\"]\n  http://nutch.apache.org/\n[/code]\n\n\n-Edit file $NUTCH_ROOT/ivy/ivy.xml\n[code lang=\"xml\"]\n \n\n\n-Edit file $NUTCH_ROOT/ivy/ivysettings.xml\n[code lang=\"xml\"]\n #\n\n\n\n[/code]\n\n\n-Edit file $NUTCH_ROOT/conf/nutch-site.xml\n[code lang=\"xml\"]\n\n\n\n\n   \nplugin.folders\n\n   \n$NUTCH_ROOT/build/plugins\n\n \n\n\n\n        \nhttp.agent.name\n\n        \nmycrawlername\n\n        \n\n    \n\n    \n\n        \nhttp.robots.agents\n\n        \nmycrawlername\n\n        \n\n    \n\n    \n\n        \nstorage.data.store.class\n\n        \norg.apache.gora.hbase.store.HBaseStore\n\n    \n\n    \n\n        \nplugin.includes\n\n        \n\n        \n\n            protocol-http|protocol-httpclient|urlfilter-regex|parse-(text|tika|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-elastic|index-metadata|index-more\n        \n\n    \n\n    \n\n        \ndb.ignore.external.links\n\n        \ntrue\n\n        \n\n    \n\n\n\n\n\n\n  \nelastic.host\n\n  \nlocalhost\n\n  \nThe hostname to send documents to using TransportClient.\n  Either host and port must be defined or cluster.\n  \n\n\n\n\n\n  \nelastic.port\n\n  \n9300\n\n  \n\n  The port to connect to using TransportClient.\n  \n\n\n\n\n\n  \nelastic.index\n\n  \nnutch\n\n  \n\n  The name of the elasticsearch index. Will normally be autocreated if it\n  doesn't exist.\n  \n\n\n\n\n\n\n\n        \nhttp.proxy.host\n\n        \n192.168.80.1\n\n    \n\n    \n\n        \nhttp.proxy.port\n\n        \n8080\n\n    \n\n    \n\n        \nhttp.proxy.username\n\n        \nuser1\n\n    \n\n    \n\n        \nhttp.proxy.password\n\n        \nuser1\n\n    \n\n\n\n[/code]\n\n\n-Edit file file $NUTCH_ROOT/conf/gora.property\n[code lang=\"xml\"]\n gora.datastore.default=org.apache.gora.hbase.store.HBaseStore\n[/code]\n\n\n-Build nucth\n[code lang=\"xml\"]\n ant runtime\n or\n ant eclipse -verbose\n[/code]\n\n\n-Cread file links\n\n\n-Runn nutch\n[code lang=\"xml\"]\n cd $NUTCH_ROOT/runtime/local/bin\n run inject : ./nutch inject file:///$PATH_LIKNS\n run generate : ./nutch generate -topN 10\n run fetch : ./nutch fetch -all\n run parse : ./nutch parse -all\n run updatedb : ./nutch updatedb -all\n[/code]\n\n\n-Downloads and extract elastic\n[code lang=\"xml\"]\n https://www.elastic.co/downloads/elasticsearch\n[/code]\n\n\n-Run elastic\n[code lang=\"xml\"]\ncd $ELASTIC/bin\n./elasticsearch\n[/code]\n\n\n-Index data in elastic\n[code lang=\"xml\"]\n cd $NUTCH_ROOT/runtime/bin\n run index : ./nutch index -all\n[/code]\n\n\n5.Run nutch intellij\n\n\nChange \n$NUTCH_ROOT/runtime/local/conf/hbase-site.xml\n\n[code lang=\"xml\"]\n\n\n\n\n\nhbase.rootdir\n\n\nfile:///home/hainv/Downloads/crawler/data\n\n\n\n\n\n\nhbase.cluster.distributed\n\n\nfalse\n\n\n\n\n\n\nhbase.zookeeper.quorum\n\n\nhbase.io\n\n\n\n\n\n\nzookeeper.znode.parent\n\n\n/hbase-unsecure\n\n\n\n\n\n\nhbase.rpc.timeout\n\n\n2592000000\n\n\n\n\n\n[/code]\n\n\nNutch plugin intellij\n\n\n1.Structure nutch :\n[1]\n\n\n2.Run nutch intellij\n\n\nDownloads nucth2.3:\nhttp://nutch.apache.org/downloads.html\n\n Editing file $NUTCH_ROOT/ivy/ivysettings.xml\n[code lang=\"xml\"]\n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n    \n\n    \n\n    \n\n     \n\n\nchain name=\"default\" dual=\"true\"\n\n  \nresolver ref=\"local\"/\n\n  \nresolver ref=\"maven2\"/\n\n  \nresolver ref=\"sonatype\"/\n\n  \nresolver ref=\"apache-snapshot\"/\n\n\n/chain\n\n\nchain name=\"internal\"\n\n  \nresolver ref=\"local\"/\n\n\n/chain\n\n\nchain name=\"external\"\n\n  \nresolver ref=\"maven2\"/\n\n  \nresolver ref=\"sonatype\"/\n\n\n/chain\n\n\nchain name=\"external-and-snapshots\"\n\n  \nresolver ref=\"maven2\"/\n\n  \nresolver ref=\"apache-snapshot\"/\n\n  \nresolver ref=\"sonatype\"/\n\n\n/chain\n\n\nchain name=\"restletchain\"\n\n  \nresolver ref=\"restlet\"/\n\n\n/chain\n\n\n\n\n\n  \n\n    \n\n    \n\n    \n\n  \n\n\n\n[/code]\n\n\nEditing file $NUTCH_ROOT/ivy/ivy.xml\n[code lang=\"xml\"]\n\n\n\nEditing file $NUCTH_ROOT/conf/gora.properties\n[code lang=\"xml\"]\ngora.datastore.default=org.apache.gora.hbase.store.HBaseStore\n[/code]\n\n\nEditing file $NUTCH_ROOT/conf/nutch_site.xml\n[code lang=\"xml\"]\n\n\n\n\n   \nplugin.folders\n\n   \n$NUTCH_ROOT/build/plugins\n\n \n\n\n\n        \nhttp.agent.name\n\n        \nmycrawlername\n\n        \n\n    \n\n    \n\n        \nhttp.robots.agents\n\n        \nmycrawlername\n\n        \n\n    \n\n    \n\n        \nstorage.data.store.class\n\n        \norg.apache.gora.hbase.store.HBaseStore\n\n    \n\n    \n\n        \nplugin.includes\n\n        \n\n        \n\n            protocol-httpclient|urlfilter-regex|parse-(text|tika|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-elastic\n        \n\n    \n\n    \n\n        \ndb.ignore.external.links\n\n        \ntrue\n\n        \n\n    \n\n    \n\n        \nelastic.host\n\n        \nlocalhost\n\n        \n\n    \n\n\n\n        \nhttp.proxy.host\n\n        \n192.168.80.1\n\n        \nThe proxy hostname. If empty, no proxy is used.\n\n    \n\n    \n\n        \nhttp.proxy.port\n\n        \n8080\n\n        \nThe proxy port.\n\n    \n\n    \n\n        \nhttp.proxy.username\n\n        \nuser1\n\n        \nUsername for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic,\n            digest\n            and/or NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of\n            'plugin.includes'\n            property. NOTE: For NTLM authentication, do not prefix the username with the domain, i.e. 'susam' is correct\n            whereas\n            'DOMAINsusam' is incorrect.\n        \n\n    \n\n    \n\n        \nhttp.proxy.password\n\n        \nuser1\n\n        \nPassword for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic,\n            digest\n            and/or NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of\n            'plugin.includes'\n            property.\n        \n\n    \n\n\n\n[/code]\n\n\nEditing file $NUCTH_ROOT/conf/hbase-site.xml\n[code lang=\"xml\"]\n\n\n    \n\n        \nhbase.rootdir\n\n        \nfile:///home/rombk/Downloads/database\n\n    \n\n    \n\n        \nhbase.cluster.distributed\n\n        \nfalse\n\n    \n\n    \n\n        \nhbase.zookeeper.quorum\n\n        \nhbase.io\n\n    \n\n    \n\n        \nzookeeper.znode.parent\n\n        \n/hbase-unsecure\n\n    \n\n    \n\n        \nhbase.rpc.timeout\n\n        \n2592000000\n\n    \n\n\n\n[/code]\n\n\nRun terminal\n[code lang=\"xml\"]\n ant eclipse -verbose\n[/code]\n\n\nImport nucth intellij\n[code lang=\"xml\"]\n\n\n[/code]\n\n\n3.Run plugin creativecommons\n\n\nSample plugins that parse and index Creative Commons medadata.\n1\n\nStep 1. Create folder creativecommons in path \n$NUTCH_HOME/out/test/\n\n\nStep 2. Create file \nnutch-site.xml\n in folder \n$NUTCH_HOME/out/test/creativecommons\n  and add content\n\n\n[code lang=\"xml\"]\n\n?xml version=\"1.0\"?\n\n\n?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\n\n\n\n\n\n\n\n   \nplugin.folders\n\n   \n$NUTCH_HOME/build/plugins\n\n \n\n\n\n   \nhttp.agent.name\n\n   \nmycrawlername\n\n\n\n\n\n\n\n   \nhttp.robots.agents\n\n   \nmycrawlername\n\n\n\n\n\n\n\n   \nstorage.data.store.class\n\n   \norg.apache.gora.hbase.store.HBaseStore\n\n\n\n\n\n   \nplugin.includes\n\n  \n\n  \nindexer-elastic|creativecommons|parse-html\n\n\n\n\n\n   \ndb.ignore.external.links\n\n   \ntrue\n\n\n\n\n\n\n\n   \nelastic.host\n\n   \nlocalhost\n\n\n\n\n\n\n\n\n\n   \nhttp.proxy.host\n\n   \n\n   \nThe proxy hostname. If empty, no proxy is used.\n\n\n\n\n\n   \nhttp.proxy.port\n\n   \n\n   \nThe proxy port.\n\n\n\n\n\n   \nhttp.proxy.username\n\n   \n\n   \nUsername for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic,\ndigest\nand/or NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of\n'plugin.includes'\nproperty. NOTE: For NTLM authentication, do not prefix the username with the domain, i.e. 'susam' is correct\nwhereas\n'DOMAINsusam' is incorrect.\n     \n\n\n\n\n\n   \nhttp.proxy.password\n\n   \n\n   \nPassword for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic,\ndigest\nand/or NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of\n'plugin.includes'\nproperty.\n    \n\n\n\n\n\n[/code]\n\n\n2.Run plugin feed\n\n\nPlugin feed parsing of rss\nError : Parsing of RSS feeds fails (tejasp) \n [2] \n and read file $NUTCH_ROOT/CHANFES.txt\n\n\nNutch Plugins\n\n\nhttp://florianhartl.com/nutch-plugin-tutorial.html\n\n\nWeb Scrapping\n\n\nWhat are some good free web scrapers / scraping techniques?\n\n\nImport.io | Web Data Platform \n Free Web Scraping Tool is a very useful tool and very easy to use. Import can operate in \u201cMagic\u201d mode where you point it at a URL and it slices and dices the content to produce a table automatically. The \"Magic Api\" page also provides options for re-running the query and downloading the results in JSON or tab-separated variable format.\n\n\ncommoncrawl\n\n\n[Feed Aggregators]\n\n\n[API]", 
            "title": "Apache Nutch"
        }, 
        {
            "location": "/nutch/#nutch", 
            "text": "Highly extensible, highly scalable Web crawler  1  Nutch is a well matured, production ready Web crawler. Nutch 1.x enables fine grained configuration, relying on Apache Hadoop\u2122 data structures, which are great for batch processing.", 
            "title": "Nutch"
        }, 
        {
            "location": "/nutch/#history", 
            "text": "", 
            "title": "History"
        }, 
        {
            "location": "/nutch/#61-extension-points", 
            "text": "In writing a plugin, you're actually providing one or more extensions of the existing extension-points . The core Nutch extension-points are themselves defined in a plugin, the NutchExtensionPoints plugin (they are listed in the NutchExtensionPoints plugin.xml file). Each extension-point defines an interface that must be implemented by the extension. The core extension points are:  \n   \n     \n      Point\n     \n\n     \n      Description\n     \n\n     \n      Example\n     \n   \n\n   \n     \n      IndexWriter\n     \n\n     \n      Writes crawled data to a specific indexing backends (Solr, ElasticSearch, a CVS file, etc.).\n     \n\n     \n     \n   \n\n   \n     \n      IndexingFilter\n     \n\n     \n      Permits one to add metadata to the indexed fields. All plugins found which implement this extension point are run sequentially on the parse (from javadoc).\n     \n\n     \n     \n   \n\n   \n     \n      Parser\n     \n\n     \n      Parser implementations read through fetched documents in order to extract data to be indexed. This is what you need to implement if you want Nutch to be able to parse a new type of content, or extract more data from currently parseable content.\n     \n\n     \n     \n   \n\n   \n     \n      HtmlParseFilter\n     \n\n     \n      Permits one to add additional metadata to HTML parses (from javadoc).\n     \n\n     \n     \n   \n\n   \n     \n      Protocol\n     \n\n     \n      Protocol implementations allow Nutch to use different protocols (ftp, http, etc.) to fetch documents.\n     \n\n     \n     \n   \n\n   \n     \n      URLFilter\n     \n\n     \n      URLFilter implementations limit the URLs that Nutch attempts to fetch. The RegexURLFilter distributed with Nutch provides a great deal of control over what URLs Nutch crawls, however if you have very complicated rules about what URLs you want to crawl, you can write your own implementation.\n     \n\n     \n     \n   \n\n   \n     \n      URLNormalizer\n     \n\n     \n      Interface used to convert URLs to normal form and optionally perform substitutions.\n     \n\n     \n     \n   \n\n   \n     \n      ScoringFilter\n     \n\n     \n      A contract defining behavior of scoring plugins. A scoring filter will manipulate scoring variables in CrawlDatum and in resulting search indexes. Filters can be chained in a specific order, to provide multi-stage scoring adjustments.\n     \n\n     \n     \n   \n\n   \n     \n      SegmentMergeFilter\n     \n\n     \n      Interface used to filter segments during segment merge. It allows filtering on more sophisticated criteria than just URLs. In particular it allows filtering based on metadata collected while parsing page.", 
            "title": "6.1 Extension Points"
        }, 
        {
            "location": "/nutch/#63-project-structure-of-a-plugin", 
            "text": "[code lang=\"text\"]\nplugin-name\nplugin.xml # file that tells Nutch about the plugin. build.xml # file that tells ant how to build the plugin. ivy.xml # file that describes any dependencies required by the plugin. src org apache nutch indexer uml-meta # source folder URLMetaIndexingFilter.java scoring uml-meta # source folder URLMetaScoringFilter.java test org apache nutch indexer uml-meta # test folder URLMetaIndexingFilterTest.java scoring uml-meta # test folder URLMetaScoringFilterTest.java\n[/code]  \n   http://nutch.apache.org/ \u00a0 \u21a9   \n   https://sites.google.com/site/nutch1936/home/introduction \u00a0 \u21a9   \n   Web Crawling with Apache Nutch \u00a0 \u21a9   \n   Introduction to Nutch, Part 1: Crawling \u00a0 \u21a9   \n   Nutch \u2013 How It Works \u00a0 \u21a9   \n   Nutch 2.3 + ElasticSearch 1.7 + HBase 0.94 Setup \u00a0 \u21a9   \n   AboutPlugins \u00a0 \u21a9   \u21a9", 
            "title": "6.3 Project structure of a plugin"
        }, 
        {
            "location": "/nutch/#config-nutch-run-intellij", 
            "text": "", 
            "title": "Config nutch run intellij"
        }, 
        {
            "location": "/nutch/#copy-file", 
            "text": "[code]\n copy all the files in the runtime/conf on out/test/apache-Nutch-2.3 and out/production/apache-Nutch-2.3\n[/code]", 
            "title": "Copy file"
        }, 
        {
            "location": "/nutch/#add-these-lines-to-file-nutch_srcouttestnutch-sitexml", 
            "text": "[code] \n    plugin.folders \n    /build/plugins \n  \n[/code]", 
            "title": "add these lines to file $NUTCH_SRC/out/test/nutch-site.xml"
        }, 
        {
            "location": "/nutch/#run-nutch-in-intellij", 
            "text": "[code]\nRun- Edit Configurations...- add path agrs:path to file list links crawler\n[/code]", 
            "title": "Run nutch in intellij"
        }, 
        {
            "location": "/nutch/#crawler", 
            "text": "Conferences  Web Search and Data Mining  Challenge  Static Crawler   Apache Nutch   Dynamic Crawler   nutch-selenium   Intelligent Extractor   boilerpipe  Web Content Extraction Through Machine Learning   Priority Crawler  Social Crawler", 
            "title": "Crawler"
        }, 
        {
            "location": "/nutch/#a-new-approach-to-dynamic-crawler", 
            "text": "Build a crawler system for dynamic websites is not easy task. While you can use a web browser automator (like  selenium ), or event when you can integrate selenium with nutch (by using  nutch-selenium ). These solutions are still hard to develop, hard to test and hard to manage sessions because we still \"translate\" our process to languages (such as java or python)  I suppose a new approach for this problem. Instead of using a  web browser automator, we can inject native javascript codes into browser (via extension or add-on).The advantages of this approach is we can easily inject third party libraries (like  jquery  (for dom selector),  Run.js  (for complicated process) and APIs that supported by browsers). And we can take advance of debugging tool and testing framework in javascript world.  If you want to know about more details, feel free to  contact me .", 
            "title": "A New Approach to Dynamic Crawler"
        }, 
        {
            "location": "/nutch/#dev-nutch-in-intellij", 
            "text": "Receipts:  IntellIJ 14 ,  Apache Nutch 2.3  1.  Get Nutch source  [code lang=\"shell\"]\nwget http://www.eu.apache.org/dist/nutch/2.3/apache-nutch-2.3-src.tar.gz\ntar -xzf apache-nutch-2.3-src.tar.gz\n[/code]  2.  Import Nutch source in IntellIJ  [wonderplugin_slider id=\"1\"]   3.  Get Dependencies by Ant  [wonderplugin_slider id=\"3\"]   4.  Import Dependencies to IntellIJ  [wonderplugin_slider id=\"4\"]", 
            "title": "Dev Nutch in Intellij"
        }, 
        {
            "location": "/nutch/#nutch-dev", 
            "text": "1.Intasll java in ubuntu  -Downloads java version .zip\n[code lang=\"xml\"]\n http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html\n[/code]  -Create folder jvm\n[code lang=\"xml\"]\n sudo mkdir /usr/lib/jvm/\n[/code]  -Cd to folder downloads java version .zip\n[code lang=\"xml\"]\n sudo mv jdk1.7.0_x/ /usr/lib/jvm/jdk1.7.0_x\n[/code]  -Run command line\n[code lang=\"xml\"]\n  sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.7.0_x/jre/bin/java 0\n[/code]  -Tets version java\n[code lang=\"xml\"]\n  java -version\n[/code]  2.Intasll ant in ubuntu  -Downloads ant\n[code lang=\"xml\"]\nhttp://ant.apache.org/manualdownload.cgi\n[/code]  -Add path ant vao file environment\n[code lang=\"xml\"]\n sudo nano /etc/environment\n $ANT_ROOT/bin\n[/code]  -Run command line\n[code lang=\"xml\"]\nsource /etc/environment\nant -version\n[/code]  3.Intasll hbase in ubuntu  -Downloads and extract hbase 0.94.27\n[code lang=\"xml\"]\n  https://archive.apache.org/dist/hbase/hbase-0.94.27/\n[/code]  -Edit file $HABSE_ROOT/conf/hbase-site.xml\n[code lang=\"xml\"]\n  \n   \n     hbase.rootdir \n     file:///$PATH_DATA_BASE/database \n   \n   \n     hbase.cluster.distributed \n     false \n   \n   \n     hbase.zookeeper.quorum \n     hbase.io \n   \n   \n     zookeeper.znode.parent \n     /hbase-unsecure \n   \n   \n     hbase.rpc.timeout \n     2592000000 \n    \n[/code]  -Edit file $HBASE_ROOT/conf/hbase-env.sh\n[code lang=\"xml\"]\n  export JAVA_HOME=$PATH_JAVA_HOME\n[/code]  -Edit file $HBASE_ROOT/conf/regionservers\n[code lang=\"xml\"]\nhbase.io.nutch\n[/code]  -Edit file hosts in ubuntu\n[code lang=\"xml\"]\n  sudo nano /etc/hosts\n  {ip} hbase.io.nutch\n[/code]  -Edit file hostname in ubuntu\n[code lang=\"xml\"]\n sudo nano /etc/hostname\n hbase.io.nutch\n[/code]  -Run and stop hbase in ubuntu\n[code]\n Run hbase : cd $HBASE_ROOT/bin ./start-hbase.sh\n Stop hbase: cd $HBASE_ROOT/bin ./stop-hbase.sh\n[/code]  *Error in intasll hbase\n[code lang=\"xml\"]\n- Error regionserver localhost(Edit file hosts and file host name)\n- Error client no remote server intasll hbase(Turn off file firewall)\n[/code]  4.Build nutch in ant  -Downloads and extract nutch\n[code lang=\"xml\"]\n  http://nutch.apache.org/\n[/code]  -Edit file $NUTCH_ROOT/ivy/ivy.xml\n[code lang=\"xml\"]\n   -Edit file $NUTCH_ROOT/ivy/ivysettings.xml\n[code lang=\"xml\"]\n #  \n[/code]  -Edit file $NUTCH_ROOT/conf/nutch-site.xml\n[code lang=\"xml\"]  \n    plugin.folders \n    $NUTCH_ROOT/build/plugins \n   \n         http.agent.name \n         mycrawlername \n         \n     \n     \n         http.robots.agents \n         mycrawlername \n         \n     \n     \n         storage.data.store.class \n         org.apache.gora.hbase.store.HBaseStore \n     \n     \n         plugin.includes \n         \n         \n            protocol-http|protocol-httpclient|urlfilter-regex|parse-(text|tika|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-elastic|index-metadata|index-more\n         \n     \n     \n         db.ignore.external.links \n         true \n         \n       \n   elastic.host \n   localhost \n   The hostname to send documents to using TransportClient.\n  Either host and port must be defined or cluster.\n     \n   elastic.port \n   9300 \n   \n  The port to connect to using TransportClient.\n     \n   elastic.index \n   nutch \n   \n  The name of the elasticsearch index. Will normally be autocreated if it\n  doesn't exist.\n      \n         http.proxy.host \n         192.168.80.1 \n     \n     \n         http.proxy.port \n         8080 \n     \n     \n         http.proxy.username \n         user1 \n     \n     \n         http.proxy.password \n         user1 \n      \n[/code]  -Edit file file $NUTCH_ROOT/conf/gora.property\n[code lang=\"xml\"]\n gora.datastore.default=org.apache.gora.hbase.store.HBaseStore\n[/code]  -Build nucth\n[code lang=\"xml\"]\n ant runtime\n or\n ant eclipse -verbose\n[/code]  -Cread file links  -Runn nutch\n[code lang=\"xml\"]\n cd $NUTCH_ROOT/runtime/local/bin\n run inject : ./nutch inject file:///$PATH_LIKNS\n run generate : ./nutch generate -topN 10\n run fetch : ./nutch fetch -all\n run parse : ./nutch parse -all\n run updatedb : ./nutch updatedb -all\n[/code]  -Downloads and extract elastic\n[code lang=\"xml\"]\n https://www.elastic.co/downloads/elasticsearch\n[/code]  -Run elastic\n[code lang=\"xml\"]\ncd $ELASTIC/bin\n./elasticsearch\n[/code]  -Index data in elastic\n[code lang=\"xml\"]\n cd $NUTCH_ROOT/runtime/bin\n run index : ./nutch index -all\n[/code]  5.Run nutch intellij  Change  $NUTCH_ROOT/runtime/local/conf/hbase-site.xml \n[code lang=\"xml\"]   hbase.rootdir  file:///home/hainv/Downloads/crawler/data    hbase.cluster.distributed  false    hbase.zookeeper.quorum  hbase.io    zookeeper.znode.parent  /hbase-unsecure    hbase.rpc.timeout  2592000000   \n[/code]", 
            "title": "Nutch Dev"
        }, 
        {
            "location": "/nutch/#nutch-plugin-intellij", 
            "text": "", 
            "title": "Nutch plugin intellij"
        }, 
        {
            "location": "/nutch/#1structure-nutch-1", 
            "text": "", 
            "title": "1.Structure nutch :[1]"
        }, 
        {
            "location": "/nutch/#2run-nutch-intellij", 
            "text": "Downloads nucth2.3: http://nutch.apache.org/downloads.html \n Editing file $NUTCH_ROOT/ivy/ivysettings.xml\n[code lang=\"xml\"] \n   \n   \n   \n   \n   \n   \n   \n   \n   \n     \n     \n     \n       chain name=\"default\" dual=\"true\" \n   resolver ref=\"local\"/ \n   resolver ref=\"maven2\"/ \n   resolver ref=\"sonatype\"/ \n   resolver ref=\"apache-snapshot\"/  /chain  chain name=\"internal\" \n   resolver ref=\"local\"/  /chain  chain name=\"external\" \n   resolver ref=\"maven2\"/ \n   resolver ref=\"sonatype\"/  /chain  chain name=\"external-and-snapshots\" \n   resolver ref=\"maven2\"/ \n   resolver ref=\"apache-snapshot\"/ \n   resolver ref=\"sonatype\"/  /chain  chain name=\"restletchain\" \n   resolver ref=\"restlet\"/  /chain   \n   \n     \n     \n     \n    \n[/code]  Editing file $NUTCH_ROOT/ivy/ivy.xml\n[code lang=\"xml\"]  Editing file $NUCTH_ROOT/conf/gora.properties\n[code lang=\"xml\"]\ngora.datastore.default=org.apache.gora.hbase.store.HBaseStore\n[/code]  Editing file $NUTCH_ROOT/conf/nutch_site.xml\n[code lang=\"xml\"]  \n    plugin.folders \n    $NUTCH_ROOT/build/plugins \n   \n         http.agent.name \n         mycrawlername \n         \n     \n     \n         http.robots.agents \n         mycrawlername \n         \n     \n     \n         storage.data.store.class \n         org.apache.gora.hbase.store.HBaseStore \n     \n     \n         plugin.includes \n         \n         \n            protocol-httpclient|urlfilter-regex|parse-(text|tika|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|indexer-elastic\n         \n     \n     \n         db.ignore.external.links \n         true \n         \n     \n     \n         elastic.host \n         localhost \n         \n      \n         http.proxy.host \n         192.168.80.1 \n         The proxy hostname. If empty, no proxy is used. \n     \n     \n         http.proxy.port \n         8080 \n         The proxy port. \n     \n     \n         http.proxy.username \n         user1 \n         Username for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic,\n            digest\n            and/or NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of\n            'plugin.includes'\n            property. NOTE: For NTLM authentication, do not prefix the username with the domain, i.e. 'susam' is correct\n            whereas\n            'DOMAINsusam' is incorrect.\n         \n     \n     \n         http.proxy.password \n         user1 \n         Password for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic,\n            digest\n            and/or NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of\n            'plugin.includes'\n            property.\n         \n      \n[/code]  Editing file $NUCTH_ROOT/conf/hbase-site.xml\n[code lang=\"xml\"] \n     \n         hbase.rootdir \n         file:///home/rombk/Downloads/database \n     \n     \n         hbase.cluster.distributed \n         false \n     \n     \n         hbase.zookeeper.quorum \n         hbase.io \n     \n     \n         zookeeper.znode.parent \n         /hbase-unsecure \n     \n     \n         hbase.rpc.timeout \n         2592000000 \n      \n[/code]  Run terminal\n[code lang=\"xml\"]\n ant eclipse -verbose\n[/code]  Import nucth intellij\n[code lang=\"xml\"]  [/code]", 
            "title": "2.Run nutch intellij"
        }, 
        {
            "location": "/nutch/#3run-plugin-creativecommons", 
            "text": "Sample plugins that parse and index Creative Commons medadata. 1 \nStep 1. Create folder creativecommons in path  $NUTCH_HOME/out/test/  Step 2. Create file  nutch-site.xml  in folder  $NUTCH_HOME/out/test/creativecommons   and add content  [code lang=\"xml\"] ?xml version=\"1.0\"?  ?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?    \n    plugin.folders \n    $NUTCH_HOME/build/plugins \n   \n    http.agent.name \n    mycrawlername    \n    http.robots.agents \n    mycrawlername    \n    storage.data.store.class \n    org.apache.gora.hbase.store.HBaseStore   \n    plugin.includes \n   \n   indexer-elastic|creativecommons|parse-html   \n    db.ignore.external.links \n    true    \n    elastic.host \n    localhost     \n    http.proxy.host \n    \n    The proxy hostname. If empty, no proxy is used.   \n    http.proxy.port \n    \n    The proxy port.   \n    http.proxy.username \n    \n    Username for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic,\ndigest\nand/or NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of\n'plugin.includes'\nproperty. NOTE: For NTLM authentication, do not prefix the username with the domain, i.e. 'susam' is correct\nwhereas\n'DOMAINsusam' is incorrect.\n        \n    http.proxy.password \n    \n    Password for proxy. This will be used by 'protocol-httpclient', if the proxy server requests basic,\ndigest\nand/or NTLM authentication. To use this, 'protocol-httpclient' must be present in the value of\n'plugin.includes'\nproperty.\n       \n[/code]", 
            "title": "3.Run plugin creativecommons"
        }, 
        {
            "location": "/nutch/#2run-plugin-feed", 
            "text": "Plugin feed parsing of rss\nError : Parsing of RSS feeds fails (tejasp)   [2]   and read file $NUTCH_ROOT/CHANFES.txt", 
            "title": "2.Run plugin feed"
        }, 
        {
            "location": "/nutch/#nutch-plugins", 
            "text": "http://florianhartl.com/nutch-plugin-tutorial.html", 
            "title": "Nutch Plugins"
        }, 
        {
            "location": "/nutch/#web-scrapping", 
            "text": "What are some good free web scrapers / scraping techniques?  Import.io | Web Data Platform   Free Web Scraping Tool is a very useful tool and very easy to use. Import can operate in \u201cMagic\u201d mode where you point it at a URL and it slices and dices the content to produce a table automatically. The \"Magic Api\" page also provides options for re-running the query and downloading the results in JSON or tab-separated variable format.  commoncrawl  [Feed Aggregators]  [API]", 
            "title": "Web Scrapping"
        }, 
        {
            "location": "/kibana/", 
            "text": "Kibana\n\n\n\n\n\n  Kibana is an \nopen source data visualization\n plugin for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data.", 
            "title": "Kibana"
        }, 
        {
            "location": "/kibana/#kibana", 
            "text": "Kibana is an  open source data visualization  plugin for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data.", 
            "title": "Kibana"
        }, 
        {
            "location": "/logstash/", 
            "text": "Logstash\n\n\nhttps://www.digitalocean.com/community/tutorials/how-to-use-logstash-and-kibana-to-centralize-logs-on-centos-6", 
            "title": "Logstash"
        }, 
        {
            "location": "/logstash/#logstash", 
            "text": "https://www.digitalocean.com/community/tutorials/how-to-use-logstash-and-kibana-to-centralize-logs-on-centos-6", 
            "title": "Logstash"
        }, 
        {
            "location": "/neo4j/", 
            "text": "Neo4J\n\n\n\n\n\n  Neo4j is an \nopen-source graph database\n, implemented in Java. The developers describe Neo4j as \"embedded, disk-based, fully transactional Java persistence engine that stores data structured in graphs rather than in tables\". Neo4j is the most popular graph database.\n\n\n\n\nPython Client\n\n\n\npy2neo\n\n\n[code language=\"python\"]\n\n\nconnect to graph\n\n\nauthenticate(\nlocalhost:7474\n, \nneo4j\n, \npasswd\n)\ngraph = Graph(\nhttp://localhost:7474/db/data/\n)\n\n\ncreate unique\n\n\ngraph.schema.create_uniqueness_constraint('Person', 'name')\n\n\nadd nodes\n\n\ngraph.create(Node.cast('Person', {\nname\n: \nAlice\n}))\ngraph.create(Node.cast('Person', {\nname\n: \nBob\n}))\n\n\nadd relationship\n\n\nsource = graph.merge_one(\nPerson\n, \nname\n, \nAlice\n)\ntarget graph.merge_one(\nPerson\n, \nname\n, \nBob\n)\ngraph.create_unique(Relationship(source, \nFRIEND\n, target))\n\n\nupdate property\n\n\nalice = graph.merge_one(\nPerson\n, \nname\n, \nAlice\n)\nalice[\nage\n] = 30\nalice.push()\n[/code]\n\n\nGraph Algorithms\n\n\nshortestPath\n, \ndijkstra\n\n\n[code lang=\"text\"]\nPOST http://localhost:7474/db/data/node/72/paths\n\n\nHeaders\nAccept: application/json\nAuthorization: Basic bmVvNGo6cGFzc3dk\n\n\nBody\n{\n  \nto\n : \nhttp://localhost:7474/db/data/node/77\n,\n  \nmax_depth\n : 5,\n  \nrelationships\n : {\n    \ntype\n : \nFRIEND\n,\n    \ndirection\n : \nout\n\n  },\n  \nalgorithm\n : \nshortestPath\n\n}\n[/code]\n\n\nGraph Analystic\n\n\npagerank\n, \ncloseness_centrality\n, \nbetweenness_centrality\n, \ntriangle_count\n,\n\nconnected_components\n, \nstrongly_connected_components\n\n\n \nkbastani/docker-neo4j\n\n\n\n  docker run -d -p 7474:7474 -v /Users//path/to/neo4j/data:/opt/data --name graphdb kbastani/docker-neo4j\n\n\n\n\n \nkbastani/neo4j-graph-analytics/\n\n\nReferences\n\n\n\n#1\n from DB-Engines Ranking of Graph DBMS\n\n\nhttp://www.kennybastani.com/2014/11/using-apache-spark-and-neo4j-for-big.html\n\n\nBig Data Stack\n: \nHDFS\n, \nKibana\n, \nElasticSearch\n, \nNeo4J\n, \nApache Spark\n\n\nNeo4j Quick Notes\n\n\nSchema Discovery\n\n\nList all nodes label, list all relation type\n\n\n[code]\n\n\n\n\nSTART n=node(*) RETURN distinct labels(n)\n\n\nmatch n-[r]-() return distinct type(r)\n[/code]\n\n\n\n\nUI Way\n: Click to Overtab in Neo4j Browser\n\n\nSample 10 entities\n\n\n[code]\n\n\n\n\nMATCH (n:Entity) RETURN n, rand() as random ORDER BY random LIMIT 10\n[/code]\n\n\n\n\nGroup By\n\n\nhttp://www.markhneedham.com/blog/2013/02/17/neo4jcypher-sql-style-group-by-functionality/\n\n\nNeo4J: Docker\n\n\nhttps://hub.docker.com/r/library/neo4j/\n\n\n[code]\ndocker run \\\n    --detach \\\n    --publish=7474:7474 \\\n    --volume=$HOME/neo4j/data:/data \\\n    neo4j\n[/code]\n\n\nDockerfile\n[code]\nFROM neo4j:2.3.1\n[/code]\n\n\nCompose\n\n\n[code]\n  neo4j:\n    build: ./neo4j/.\n    ports:\n       - 7474:7474\n    volumes:\n       - ./data/neo4j:/data\n    environment:\n       - NEO4J_AUTH=neo4j/root\n[/code]", 
            "title": "Neo4J"
        }, 
        {
            "location": "/neo4j/#neo4j", 
            "text": "Neo4j is an  open-source graph database , implemented in Java. The developers describe Neo4j as \"embedded, disk-based, fully transactional Java persistence engine that stores data structured in graphs rather than in tables\". Neo4j is the most popular graph database.", 
            "title": "Neo4J"
        }, 
        {
            "location": "/neo4j/#connect-to-graph", 
            "text": "authenticate( localhost:7474 ,  neo4j ,  passwd )\ngraph = Graph( http://localhost:7474/db/data/ )", 
            "title": "connect to graph"
        }, 
        {
            "location": "/neo4j/#create-unique", 
            "text": "graph.schema.create_uniqueness_constraint('Person', 'name')", 
            "title": "create unique"
        }, 
        {
            "location": "/neo4j/#add-nodes", 
            "text": "graph.create(Node.cast('Person', { name :  Alice }))\ngraph.create(Node.cast('Person', { name :  Bob }))", 
            "title": "add nodes"
        }, 
        {
            "location": "/neo4j/#add-relationship", 
            "text": "source = graph.merge_one( Person ,  name ,  Alice )\ntarget graph.merge_one( Person ,  name ,  Bob )\ngraph.create_unique(Relationship(source,  FRIEND , target))", 
            "title": "add relationship"
        }, 
        {
            "location": "/neo4j/#update-property", 
            "text": "alice = graph.merge_one( Person ,  name ,  Alice )\nalice[ age ] = 30\nalice.push()\n[/code]  Graph Algorithms  shortestPath ,  dijkstra  [code lang=\"text\"]\nPOST http://localhost:7474/db/data/node/72/paths  Headers\nAccept: application/json\nAuthorization: Basic bmVvNGo6cGFzc3dk  Body\n{\n   to  :  http://localhost:7474/db/data/node/77 ,\n   max_depth  : 5,\n   relationships  : {\n     type  :  FRIEND ,\n     direction  :  out \n  },\n   algorithm  :  shortestPath \n}\n[/code]  Graph Analystic  pagerank ,  closeness_centrality ,  betweenness_centrality ,  triangle_count , connected_components ,  strongly_connected_components    kbastani/docker-neo4j  \n  docker run -d -p 7474:7474 -v /Users//path/to/neo4j/data:/opt/data --name graphdb kbastani/docker-neo4j    kbastani/neo4j-graph-analytics/", 
            "title": "update property"
        }, 
        {
            "location": "/neo4j/#neo4j-quick-notes", 
            "text": "", 
            "title": "Neo4j Quick Notes"
        }, 
        {
            "location": "/neo4j/#schema-discovery", 
            "text": "List all nodes label, list all relation type  [code]   START n=node(*) RETURN distinct labels(n)  match n-[r]-() return distinct type(r)\n[/code]   UI Way : Click to Overtab in Neo4j Browser", 
            "title": "Schema Discovery"
        }, 
        {
            "location": "/neo4j/#sample-10-entities", 
            "text": "[code]   MATCH (n:Entity) RETURN n, rand() as random ORDER BY random LIMIT 10\n[/code]", 
            "title": "Sample 10 entities"
        }, 
        {
            "location": "/neo4j/#group-by", 
            "text": "http://www.markhneedham.com/blog/2013/02/17/neo4jcypher-sql-style-group-by-functionality/", 
            "title": "Group By"
        }, 
        {
            "location": "/neo4j/#neo4j-docker", 
            "text": "https://hub.docker.com/r/library/neo4j/  [code]\ndocker run \\\n    --detach \\\n    --publish=7474:7474 \\\n    --volume=$HOME/neo4j/data:/data \\\n    neo4j\n[/code]  Dockerfile\n[code]\nFROM neo4j:2.3.1\n[/code]  Compose  [code]\n  neo4j:\n    build: ./neo4j/.\n    ports:\n       - 7474:7474\n    volumes:\n       - ./data/neo4j:/data\n    environment:\n       - NEO4J_AUTH=neo4j/root\n[/code]", 
            "title": "Neo4J: Docker"
        }, 
        {
            "location": "/elasticsearch/", 
            "text": "Elasticsearch\n\n\n\n\n\n  Elasticsearch is a \nsearch server\n based on Lucene. It provides a distributed, multitenant-capable full-text search engine with a RESTful web interface and schema-free JSON documents. Elasticsearch is developed in Java and is released as open source under the terms of the Apache License. Elasticsearch is the second most popular enterprise search engine\n\n\n\n\n1. Basic Concenpts\n\n\n\n\n\n\n\n\n\nRelational Database\n\n\nElasticsearch\n\n\n\n\n\n\n\n\n\n\n\nDatabase\n\n\nIndex\n\n\n\n\n\n\n\nTable\n\n\nType\n\n\n\n\n\n\n\nRow\n\n\nDocument\n\n\n\n\n\n\n\nColumn\n\n\nField\n\n\n\n\n\n\n\nSchema\n\n\nMapping\n\n\n\n\n\n\n\n\n\n2. Index \n Query\n\n\n\nGet all indices\n\n\n\n\n  /_stats\n\n\n\n\nSearch API \n1\n\n\n\nSearch All\n\n\n\n\n  /bank/_search?q=*\n\n\n\n\nhits.hits\n \u2013 actual array of search results (defaults to first 10 documents)\n\n\nQuery Language\n\n\n\nelasticsearch provides a full \nQuery DSL\n based on JSON to define queries.\n\n\n\n  curl -XPOST /bank/_search\n\n\n\n\n// match all, limit 10 offset 10\n{\n  \nquery\n: { \nmatch_all\n: {} },\n  \nfrom\n: 10,\n  \nsize\n: 10\n}\n\n// select fields\n{\n  \nquery\n: { \nmatch_all\n: {} },\n  _source: [\naccount_number\n, \nbalance\n]\n  \nsize\n: 10\n}\n\n// where account equals 20\n{\n  \nquery\n: { \nmatch\n: { \naccount_number\n: 20 } }\n}\n\n\n\n\nFilter\n\n\ncurl -XPOST elastic:9200/index/type/_search -d '\n{\n  \nquery\n : {\n    \nfiltered\n :\n    {\n      \nquery\n : { \nterm\n : { \nfeature\n : 1 } } ,\n      \nfilter\n : {\n        \nand\n : [\n          {\n            \nrange\n: {\n              \n_timestamp\n: {\n                \nfrom\n: 1441964671000,\n                \nto\n: 1441964672000\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n\n\n\n\nSort\n\n\ncurl -XPOST elastic:9200/index/type/_search -d '\n{\n  \nquery\n : {\n    \nfiltered\n :\n    {\n      \nquery\n : { \nterm\n : { \nfeature\n : 1 } } ,\n      \nfilter\n : {\n        \nand\n : [\n          {\n            \nrange\n: {\n              \n_timestamp\n: {\n                \nfrom\n: 1441964671000,\n                \nto\n: 1441964672000\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n\n\n\n\n3. Mapping\n\n\n\nTimestamp \n2\n\n\n\nEnable and store timestamp\n\n\n\n  curl -XPOST localhost:9200/test\n\n\n\n\n{\n\nmappings\n : {\n    \n_default_\n:{\n        \n_timestamp\n : {\n            \nenabled\n : true,\n            \nstore\n : true\n        }\n    }\n  }\n}'\n\n\n\n\nRelationships Management \n3\n \n4\n\n\n\nInner Object\n\n\n\n\n\ud83d\udc4d Easy, fast, performant\n\n\n\ud83d\udc4e No need for special queries\n\n\n\u261b Only applicable when one-to-one relationships are maintained\n\n\n\n\n\nNested\n\n\n\n\n\ud83d\udc4d Nested docs are stored in the same Lucene block as each other, which helps read/query  performance. Reading a nested doc is faster than the equivalent parent/child.\n\n\n\ud83d\udc4e Updating a single field in a nested document (parent or nested children) forces ES to reindex the entire nested document. This can be very expensive for large nested docs\n\n\n\ud83d\udc4e \u201cCross referencing\u201d nested documents is impossible\n\n\n\u261b Best suited for data that does not change frequently\n\n\n\n\n\nParent/Child\n\n\n\n\n\ud83d\udc4d Updating a child doc does not affect the parent or any other children, which can potentially save a lot of indexing on large docs\n\n\n\ud83d\udc4e Children are stored separately from the parent, but are routed to the same shard. So parent/children are slightly less performance on read/query than nested\n\n\n\ud83d\udc4e Parent/child mappings have a bit extra memory overhead, since ES maintains a \u201cjoin\u201d list in memory\n\n\n\ud83d\udc4e Sorting/scoring can be difficult with Parent/Child since the Has Child/Has Parent operations can be opaque at times\n\n\n\n\n\nDenormalization\n\n\n\n\n\ud83d\udc4d You get to manage all the relations yourself!\n\n\n\ud83d\udc4e Most flexible, most administrative overhead\n\n\n\u261b May be more or less performant depending on your setup\n\n\n\n\n\n4. Backup\n\n\n\nElastic Dump \n5\n\n\n\nTools for moving and saving indicies.\n\n\nbin/elasticdump\n  --input=http://localhost:9200/index_1\n  --output=http://localhost:9200/index_1_backup\n  --type=data\n  --scrollTime=100\n\n\n\n\nAlias \n6\n\n\n\ncurl -XPOST 'http://localhost:9200/_aliases' -d '\n{\n    \nquot;actions\nquot; : [\n        { \nquot;remove\nquot; : { \nquot;index\nquot; : \nquot;test1\nquot;, \nquot;alias\nquot; : \nquot;alias1\nquot; } },\n        { \nquot;add\nquot; : { \nquot;index\nquot; : \nquot;test1\nquot;, \nquot;alias\nquot; : \nquot;alias2\nquot; } }\n    ]\n}'\n\n\n\n\n5. Module Scripting \n7\n\n\n\nRanking\n\n\n\nRank #2 from DB-Engines Ranking of Search Engines\n\n\n\n\n\n\n\n\n\n\n\nThe Search API\n\n\n\n\n\n\nhttp://stackoverflow.com/a/17146144/772391\n\n\n\n\n\n\n\nhttp://stackoverflow.com/a/23407367/772391\n\n\n\n\n\n\n\nhttps://www.elastic.co/guide/en/elasticsearch/guide/current/modeling-your-data.html\n\n\n\n\n\n\nhttps://github.com/taskrabbit/elasticsearch-dump\n\n\n\n\n\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html\n\n\n\n\n\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html\n\n\n\n\n\n\n\n\n\n\nElasticsearch tutorial series 1: Metric Aggregations with Social Network Data\n\n\nTable of content\n\n\n\n\nAvg, Max, Min, Sum Aggregation\n\n\nCardinality Aggregation\n\n\nStats Aggregation\n\n\nExtended Stats Aggregation\n\n\nPercentile Aggregation\n\n\nPercentile Ranks Aggregation\n\n\nTop hits Aggregation\n\n\n\n\nAvg, Max, Min, Sum, Count Aggregation\n\n\nDoc: Avg Aggregation\n, \nDoc: Max Aggregation\n, \nDoc: Min Aggregation\n\n\nGet max, min, avg, sum, count about number of likes, shares, comments\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\naggs\n:{\nsum_like\n:{\nsum\n:{\nfield\n:\nnum_like\n}},\nmin_like\n:{\nmin\n:{\nfield\n:\nnum_like\n}},\navg_like\n:{\navg\n:{\nfield\n:\nnum_like\n}},\nmax_like\n:{\nmax\n:{\nfield\n:\nnum_like\n}},\nsum_share\n:{\nsum\n:{\nfield\n:\nnum_share\n}},\nmin_share\n:{\nmin\n:{\nfield\n:\nnum_share\n}},\navg_share\n:{\navg\n:{\nfield\n:\nnum_share\n}},\nmax_share\n:{\nmax\n:{\nfield\n:\nnum_share\n}},\nsum_comment\n:{\nsum\n:{\nfield\n:\nnum_comment\n}},\nmin_comment\n:{\nmin\n:{\nfield\n:\nnum_comment\n}},\navg_comment\n:{\navg\n:{\nfield\n:\nnum_comment\n}},\nmax_comment\n:{\nmax\n:{\nfield\n:\nnum_comment\n}}}}\n\n\n\n\nRequest\n\n\n{\n\naggregations\n: {\n      \navg_comment\n: {\n         \nvalue\n: 75.23860589812332\n      },\n      \nmin_like\n: {\n         \nvalue\n: 0\n      },\n      \navg_like\n: {\n         \nvalue\n: 1761974365266098.2\n      },\n      \nsum_like\n: {\n         \nvalue\n: 3238508883359088600\n      },\n      \nmax_share\n: {\n         \nvalue\n: 30407\n      },\n      \nmax_comment\n: {\n         \nvalue\n: 11000\n      },\n      \nsum_share\n: {\n         \nvalue\n: 117844\n      },\n      \nmax_like\n: {\n         \nvalue\n: 2751488761761411000\n      },\n      \navg_share\n: {\n         \nvalue\n: 250.19957537154988\n      },\n      \nsum_comment\n: {\n         \nvalue\n: 28064\n      },\n      \nmin_comment\n: {\n         \nvalue\n: 2\n      },\n      \nmin_share\n: {\n         \nvalue\n: 1\n      }\n   }\n}\n\n\n\n\nCardinality Aggregation\n\n\nCardinality Aggregation\n\n\nGet total of users\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\n    \naggs\n : {\n        \nnum_authors\n : { \ncardinality\n : { \nfield\n : \nfrom.fb_id\n } }\n    }\n}\n\n\n\n\nResponse\n\n\n{\n   \naggregations\n: {\n      \nnum_authors\n: {\n         \nvalue\n: 7385\n      }\n   }\n}\n\n\n\n\nStats Aggregation\n\n\nDoc: Stats Aggregation\n\n\nBasic Stats of like, share \n comment\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\n    \naggs\n : {\n        \nshares\n : { \nstats\n : { \nfield\n : \nnum_share\n } },\n        \nlikes\n : { \nstats\n : { \nfield\n : \nnum_like\n } },\n        \ncomments\n : { \nstats\n : { \nfield\n : \nnum_comment\n } }\n    }\n}\n\n\n\n\nResponse\n\n\n{\n   \naggregations\n: {\n      \nshares\n: {\n         \ncount\n: 471,\n         \nmin\n: 1,\n         \nmax\n: 30407,\n         \navg\n: 250.19957537154988,\n         \nsum\n: 117844\n      },\n      \ncomments\n: {\n         \ncount\n: 373,\n         \nmin\n: 2,\n         \nmax\n: 11000,\n         \navg\n: 75.23860589812332,\n         \nsum\n: 28064\n      },\n      \nlikes\n: {\n         \ncount\n: 1838,\n         \nmin\n: 0,\n         \nmax\n: 2751488761761411000,\n         \navg\n: 1761974365266098.2,\n         \nsum\n: 3238508883359088600\n      }\n   }\n}\n\n\n\n\nExtended Stats Aggregation\n\n\nExtended Stats Aggregation\n\n\nStats of like, share \n comment with more metrics, such as sum, std_deviation, std_deviation_bounds, variance\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\n    \naggs\n : {\n        \nlike_stats\n : { \nextended_stats\n : { \nfield\n : \nnum_like\n } },\n        \nshare_stats\n : { \nextended_stats\n : { \nfield\n : \nnum_share\n } },\n        \ncomment_stats\n : { \nextended_stats\n : { \nfield\n : \nnum_comment\n } }\n    }\n}\n\n\n\n\nResponse\n\n\n{\n   \naggregations\n: {\n      \nlike_stats\n: {\n         \ncount\n: 1838,\n         \nmin\n: 0,\n         \nmax\n: 2751488761761411000,\n         \navg\n: 1761974365266098.2,\n         \nsum\n: 3238508883359088600,\n         \nsum_of_squares\n: 7.667542671405507e+36,\n         \nvariance\n: 4.168572634260795e+33,\n         \nstd_deviation\n: 64564484310345070,\n         \nstd_deviation_bounds\n: {\n            \nupper\n: 130890942985956240,\n            \nlower\n: -127366994255424050\n         }\n      },\n      \nshare_stats\n: {\n         \ncount\n: 471,\n         \nmin\n: 1,\n         \nmax\n: 30407,\n         \navg\n: 250.19957537154988,\n         \nsum\n: 117844,\n         \nsum_of_squares\n: 1769467022,\n         \nvariance\n: 3694230.367812983,\n         \nstd_deviation\n: 1922.0380765773043,\n         \nstd_deviation_bounds\n: {\n            \nupper\n: 4094.2757285261587,\n            \nlower\n: -3593.8765777830586\n         }\n      },\n      \ncomment_stats\n: {\n         \ncount\n: 373,\n         \nmin\n: 2,\n         \nmax\n: 11000,\n         \navg\n: 75.23860589812332,\n         \nsum\n: 28064,\n         \nsum_of_squares\n: 131531392,\n         \nvariance\n: 346970.2299304962,\n         \nstd_deviation\n: 589.0417896299856,\n         \nstd_deviation_bounds\n: {\n            \nupper\n: 1253.3221851580945,\n            \nlower\n: -1102.844973361848\n         }\n      }\n   }\n}\n\n\n\n\nPercentiles Aggregation\n\n\nDoc: Percentiles Aggregation\n\n\nComment, Like, Share Percentiles\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\naggs\n:{\nlike_percentiles\n:{\npercentiles\n:{\nfield\n:\nnum_like\n}},\nshare_percentiles\n:{\npercentiles\n:{\nfield\n:\nnum_share\n}},\ncomment_percentiles\n:{\npercentiles\n:{\nfield\n:\nnum_comment\n}}}}\n\n\n\n\nResponse\n\n\n{\n\naggregations\n: {\n      \nlike_percentiles\n: {\n         \nvalues\n: {\n            \n1.0\n: 0,\n            \n5.0\n: 0,\n            \n25.0\n: 4,\n            \n50.0\n: 18.35,\n            \n75.0\n: 72.53579545454545,\n            \n95.0\n: 71343.74999999999,\n            \n99.0\n: 4338260523723.276\n         }\n      },\n      \ncomment_percentiles\n: {\n         \nvalues\n: {\n            \n1.0\n: 2,\n            \n5.0\n: 2,\n            \n25.0\n: 5,\n            \n50.0\n: 10,\n            \n75.0\n: 26,\n            \n95.0\n: 139.39999999999998,\n            \n99.0\n: 1000\n         }\n      },\n      \nshare_percentiles\n: {\n         \nvalues\n: {\n            \n1.0\n: 1,\n            \n5.0\n: 1,\n            \n25.0\n: 1,\n            \n50.0\n: 4,\n            \n75.0\n: 25,\n            \n95.0\n: 251.5,\n            \n99.0\n: 5560.3\n         }\n      }\n   }\n}\n\n\n\n\nLike Percentiles with custom percents\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\n   \naggs\n: {\n      \nshare_percentiles\n: {\n         \npercentiles\n: {\n            \nfield\n: \nnum_share\n,\n            \npercents\n: [0, 10, 80, 90, 95]\n         }\n      }\n   }\n}\n\n\n\n\nResponse\n\n\n{\n   \naggregations\n: {\n      \nshare_percentiles\n: {\n         \nvalues\n: {\n            \n0.0\n: 1,\n            \n10.0\n: 1,\n            \n80.0\n: 37.33333333333333,\n            \n90.0\n: 97,\n            \n95.0\n: 251.5\n         }\n      }\n   }\n}\n\n\n\n\nPercentile Ranks Aggregation\n\n\nDoc: Percentile Ranks Aggregation\n\n\nHow like, share, comment distribute\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\n   \naggs\n: {\n      \nlike_percentile_ranks\n: {\n         \npercentile_ranks\n: {\n            \nfield\n: \nnum_like\n,\n            \nvalues\n: [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      },\n      \nshare_percentile_ranks\n: {\n         \npercentile_ranks\n: {\n            \nfield\n: \nnum_share\n,\n            \nvalues\n: [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      },\n      \ncomment_percentile_ranks\n: {\n         \npercentile_ranks\n: {\n            \nfield\n: \nnum_comment\n,\n            \nvalues\n: [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      }\n   }\n}\n\n\n\n\nResponse\n\n\n{\n   \naggregations\n: {\n      \nshare_percentile_ranks\n: {\n         \nvalues\n: {\n            \n10.0\n: 60.438782731776364,\n            \n100.0\n: 89.91507430997878,\n            \n1000.0\n: 97.37406386327386,\n            \n10000.0\n: 99.31579836222765,\n            \n1000000.0\n: 100,\n            \n1.0E7\n: 100\n         }\n      },\n      \nlike_percentile_ranks\n: {\n         \nvalues\n: {\n            \n10.0\n: 39.281828073993466,\n            \n100.0\n: 79.39530545624125,\n            \n1000.0\n: 90.98349676683587,\n            \n10000.0\n: 94.14527905373414,\n            \n1000000.0\n: 95.9014681663581,\n            \n1.0E7\n: 96.57661015941164\n         }\n      },\n      \ncomment_percentile_ranks\n: {\n         \nvalues\n: {\n            \n10.0\n: 49.865951742627345,\n            \n100.0\n: 92.18395545473294,\n            \n1000.0\n: 98.92761394101876,\n            \n10000.0\n: 99.56773202397807,\n            \n1000000.0\n: 100,\n            \n1.0E7\n: 100\n         }\n      }\n   }\n}\n\n\n\n\n\n\nAs we can see, only 0.7% posts have more than 10k shares, onley 0.04% posts have more than 10k comment, but there is an odd here. 4.1% posts have more than 1M like (WHAT!!!). We can spot some strange here.\n\n\n\n\nTop hits Aggregation\n\n\nDoc: Top hits Aggregation\n\n\nExample\n\n\nRequest\n\n\n\n\n\nResponse\n\n\n{\n\n\n\n\n\nAn Aggregation\n\n\nDoc: Link\n\n\nConfig\n\n\nelasticsearch.yml\n\n\ndiscovery.zen.minimum_master_nodes: 1\ndiscovery.zen.ping.multicast.enabled: false\ndiscovery.zen.ping.unicast.hosts: [\nlocalhost\n]\n\nnetwork.host: 0.0.0.0\nhttp.cors.enabled: true\nhttp.cors.allow-origin: '*'\nscript.inline: on\nscript.indexed: on\n\n\n\n\nDocker\n\n\nImage\n\n\nhttps://hub.docker.com/r/_/elasticsearch/\n\n\nRun\n\n\ndocker run -d -v \n$PWD/esdata\n:/usr/share/elasticsearch/data elasticsearch\n\n\n\n\nDocker Folder\n\n\nelasticsearch/\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 elasticsearch.yml\n\u2514\u2500\u2500 Dockerfile\n\n\n\n\nDockerfile\n\n\nFROM elasticsearch:2.2.0\n\nADD config/elasticsearch.yml /elasticsearch/config/elasticsearch.yml\n\n\n\n\nCompose\n\n\nelasticsearch:\n    build: ./elasticsearch/.\n    ports:\n       - 9200:9200\n       - 9300:9300\n    volumes:\n       - ./data/elasticsearch:/usr/share/elasticsearch/data\n\n\n\n\nElasticsearch: Search Ignore Accents\n\n\nThe ICU \n1\n \n2\n analysis plug-in for Elasticsearch uses the International Components for Unicode (ICU) libraries to provide a rich set of tools for dealing with Unicode. These include the icu_tokenizer, which is particularly useful for Asian languages, and a number of token filters that are essential for correct matching and sorting in all languages other than English.\n\n\nStep 1: Install ICU-Plugin \n3\n\n\ncd /usr/share/elasticsearch\nsudo bin/plugin install analysis-icu\n\n\n\n\nStep 2: Create an analyzer setting:\n\n\nsettings\n: {\n      \nanalysis\n: {\n         \nanalyzer\n: {\n            \nvnanalysis\n: {\n               \ntokenizer\n: \nicu_tokenizer\n,\n               \nfilter\n: [\n                  \nicu_folding\n,\n                  \nicu_normalizer\n\n               ]\n            }\n         }\n      }\n   }\n\n\n\n\nStep 3: Create your index, create a field with type string and analyzer is \nvnanalysis\n you have created\n\n\nkey\n: {\n     \ntype\n: \nstring\n,\n     \nanalyzer\n: \nvnanalysis\n\n}\n\n\n\n\nStep 4: Search with \nsense\n\n\nPOST /your_index/your_doc_type/_search\n{\n   \nquery\n: {\n      \nmatch\n: {\n         \nkey\n: \nkiem tra\n\n      }\n   }\n}\n\n\n\n\nES: Import CSV to Elasticsearch\n\n\nhttps://gist.github.com/clemsos/8668698\n\n\nInstall lastest Elasticdump with NVM\n\n\nAs a matter of best practice we\u2019ll update our packages:\n\n\napt-get update\n\n\n\n\nThe build-essential package should already be installed, however, we\u2019re going still going to include it in our command for installation:\n\n\napt-get install build-essential libssl-dev\n\n\n\n\nTo install or update nvm, you can use the install script using cURL:\n\n\ncurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bash\n\n\n\n\nif you have below problem or after you type \nnvm ls-remote\n command it result N/A:\n\ncurl: (77) error setting certificate verify locations:\n  CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n\n\nhead to this \n1\n:\n\n\nor Wget:\n\n\nwget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bash\n\n\n\n\nDon't forget to restart your terminal\n\n\nThen you use the following command to list available versions of nodejs\n\n\nnvm ls-remote\n\n\n\n\nTo download, compile, and install the latest v5.0.x release of node, do this:\n\n\nnvm install 5.0\n\n\n\n\nAnd then in any new shell just use the installed version:\n\n\nnvm use 5.0\n\n\n\n\nOr you can just run it:\n\n\nnvm run 5.0 --version\n\n\n\n\nOr, you can run any arbitrary command in a subshell with the desired version of node:\n\n\nnvm exec 4.2 node --version\n\n\n\n\nYou can also get the path to the executable to where it was installed:\n\n\nnvm which 5.0\n\n\n\n\nNode Version Manager\n\n\n\n\n\n\n\n\n\n\nhow to solve https problem\n\n\n\n\n\n\nICU plug-in Github\n\n\n\n\n\n\nInstalling the ICU plug-in", 
            "title": "Elasticsearch"
        }, 
        {
            "location": "/elasticsearch/#elasticsearch", 
            "text": "Elasticsearch is a  search server  based on Lucene. It provides a distributed, multitenant-capable full-text search engine with a RESTful web interface and schema-free JSON documents. Elasticsearch is developed in Java and is released as open source under the terms of the Apache License. Elasticsearch is the second most popular enterprise search engine", 
            "title": "Elasticsearch"
        }, 
        {
            "location": "/elasticsearch/#elasticsearch-tutorial-series-1-metric-aggregations-with-social-network-data", 
            "text": "Table of content   Avg, Max, Min, Sum Aggregation  Cardinality Aggregation  Stats Aggregation  Extended Stats Aggregation  Percentile Aggregation  Percentile Ranks Aggregation  Top hits Aggregation", 
            "title": "Elasticsearch tutorial series 1: Metric Aggregations with Social Network Data"
        }, 
        {
            "location": "/elasticsearch/#avg-max-min-sum-count-aggregation", 
            "text": "Doc: Avg Aggregation ,  Doc: Max Aggregation ,  Doc: Min Aggregation  Get max, min, avg, sum, count about number of likes, shares, comments  Request  POST /facebook_crawler/post/_search\n{ aggs :{ sum_like :{ sum :{ field : num_like }}, min_like :{ min :{ field : num_like }}, avg_like :{ avg :{ field : num_like }}, max_like :{ max :{ field : num_like }}, sum_share :{ sum :{ field : num_share }}, min_share :{ min :{ field : num_share }}, avg_share :{ avg :{ field : num_share }}, max_share :{ max :{ field : num_share }}, sum_comment :{ sum :{ field : num_comment }}, min_comment :{ min :{ field : num_comment }}, avg_comment :{ avg :{ field : num_comment }}, max_comment :{ max :{ field : num_comment }}}}  Request  { aggregations : {\n       avg_comment : {\n          value : 75.23860589812332\n      },\n       min_like : {\n          value : 0\n      },\n       avg_like : {\n          value : 1761974365266098.2\n      },\n       sum_like : {\n          value : 3238508883359088600\n      },\n       max_share : {\n          value : 30407\n      },\n       max_comment : {\n          value : 11000\n      },\n       sum_share : {\n          value : 117844\n      },\n       max_like : {\n          value : 2751488761761411000\n      },\n       avg_share : {\n          value : 250.19957537154988\n      },\n       sum_comment : {\n          value : 28064\n      },\n       min_comment : {\n          value : 2\n      },\n       min_share : {\n          value : 1\n      }\n   }\n}", 
            "title": "Avg, Max, Min, Sum, Count Aggregation"
        }, 
        {
            "location": "/elasticsearch/#cardinality-aggregation", 
            "text": "Cardinality Aggregation  Get total of users  Request  POST /facebook_crawler/post/_search\n{\n     aggs  : {\n         num_authors  : {  cardinality  : {  field  :  from.fb_id  } }\n    }\n}  Response  {\n    aggregations : {\n       num_authors : {\n          value : 7385\n      }\n   }\n}", 
            "title": "Cardinality Aggregation"
        }, 
        {
            "location": "/elasticsearch/#stats-aggregation", 
            "text": "Doc: Stats Aggregation  Basic Stats of like, share   comment  Request  POST /facebook_crawler/post/_search\n{\n     aggs  : {\n         shares  : {  stats  : {  field  :  num_share  } },\n         likes  : {  stats  : {  field  :  num_like  } },\n         comments  : {  stats  : {  field  :  num_comment  } }\n    }\n}  Response  {\n    aggregations : {\n       shares : {\n          count : 471,\n          min : 1,\n          max : 30407,\n          avg : 250.19957537154988,\n          sum : 117844\n      },\n       comments : {\n          count : 373,\n          min : 2,\n          max : 11000,\n          avg : 75.23860589812332,\n          sum : 28064\n      },\n       likes : {\n          count : 1838,\n          min : 0,\n          max : 2751488761761411000,\n          avg : 1761974365266098.2,\n          sum : 3238508883359088600\n      }\n   }\n}", 
            "title": "Stats Aggregation"
        }, 
        {
            "location": "/elasticsearch/#extended-stats-aggregation", 
            "text": "Extended Stats Aggregation  Stats of like, share   comment with more metrics, such as sum, std_deviation, std_deviation_bounds, variance  Request  POST /facebook_crawler/post/_search\n{\n     aggs  : {\n         like_stats  : {  extended_stats  : {  field  :  num_like  } },\n         share_stats  : {  extended_stats  : {  field  :  num_share  } },\n         comment_stats  : {  extended_stats  : {  field  :  num_comment  } }\n    }\n}  Response  {\n    aggregations : {\n       like_stats : {\n          count : 1838,\n          min : 0,\n          max : 2751488761761411000,\n          avg : 1761974365266098.2,\n          sum : 3238508883359088600,\n          sum_of_squares : 7.667542671405507e+36,\n          variance : 4.168572634260795e+33,\n          std_deviation : 64564484310345070,\n          std_deviation_bounds : {\n             upper : 130890942985956240,\n             lower : -127366994255424050\n         }\n      },\n       share_stats : {\n          count : 471,\n          min : 1,\n          max : 30407,\n          avg : 250.19957537154988,\n          sum : 117844,\n          sum_of_squares : 1769467022,\n          variance : 3694230.367812983,\n          std_deviation : 1922.0380765773043,\n          std_deviation_bounds : {\n             upper : 4094.2757285261587,\n             lower : -3593.8765777830586\n         }\n      },\n       comment_stats : {\n          count : 373,\n          min : 2,\n          max : 11000,\n          avg : 75.23860589812332,\n          sum : 28064,\n          sum_of_squares : 131531392,\n          variance : 346970.2299304962,\n          std_deviation : 589.0417896299856,\n          std_deviation_bounds : {\n             upper : 1253.3221851580945,\n             lower : -1102.844973361848\n         }\n      }\n   }\n}", 
            "title": "Extended Stats Aggregation"
        }, 
        {
            "location": "/elasticsearch/#percentiles-aggregation", 
            "text": "Doc: Percentiles Aggregation  Comment, Like, Share Percentiles  Request  POST /facebook_crawler/post/_search\n{ aggs :{ like_percentiles :{ percentiles :{ field : num_like }}, share_percentiles :{ percentiles :{ field : num_share }}, comment_percentiles :{ percentiles :{ field : num_comment }}}}  Response  { aggregations : {\n       like_percentiles : {\n          values : {\n             1.0 : 0,\n             5.0 : 0,\n             25.0 : 4,\n             50.0 : 18.35,\n             75.0 : 72.53579545454545,\n             95.0 : 71343.74999999999,\n             99.0 : 4338260523723.276\n         }\n      },\n       comment_percentiles : {\n          values : {\n             1.0 : 2,\n             5.0 : 2,\n             25.0 : 5,\n             50.0 : 10,\n             75.0 : 26,\n             95.0 : 139.39999999999998,\n             99.0 : 1000\n         }\n      },\n       share_percentiles : {\n          values : {\n             1.0 : 1,\n             5.0 : 1,\n             25.0 : 1,\n             50.0 : 4,\n             75.0 : 25,\n             95.0 : 251.5,\n             99.0 : 5560.3\n         }\n      }\n   }\n}  Like Percentiles with custom percents  Request  POST /facebook_crawler/post/_search\n{\n    aggs : {\n       share_percentiles : {\n          percentiles : {\n             field :  num_share ,\n             percents : [0, 10, 80, 90, 95]\n         }\n      }\n   }\n}  Response  {\n    aggregations : {\n       share_percentiles : {\n          values : {\n             0.0 : 1,\n             10.0 : 1,\n             80.0 : 37.33333333333333,\n             90.0 : 97,\n             95.0 : 251.5\n         }\n      }\n   }\n}", 
            "title": "Percentiles Aggregation"
        }, 
        {
            "location": "/elasticsearch/#percentile-ranks-aggregation", 
            "text": "Doc: Percentile Ranks Aggregation  How like, share, comment distribute  Request  POST /facebook_crawler/post/_search\n{\n    aggs : {\n       like_percentile_ranks : {\n          percentile_ranks : {\n             field :  num_like ,\n             values : [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      },\n       share_percentile_ranks : {\n          percentile_ranks : {\n             field :  num_share ,\n             values : [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      },\n       comment_percentile_ranks : {\n          percentile_ranks : {\n             field :  num_comment ,\n             values : [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      }\n   }\n}  Response  {\n    aggregations : {\n       share_percentile_ranks : {\n          values : {\n             10.0 : 60.438782731776364,\n             100.0 : 89.91507430997878,\n             1000.0 : 97.37406386327386,\n             10000.0 : 99.31579836222765,\n             1000000.0 : 100,\n             1.0E7 : 100\n         }\n      },\n       like_percentile_ranks : {\n          values : {\n             10.0 : 39.281828073993466,\n             100.0 : 79.39530545624125,\n             1000.0 : 90.98349676683587,\n             10000.0 : 94.14527905373414,\n             1000000.0 : 95.9014681663581,\n             1.0E7 : 96.57661015941164\n         }\n      },\n       comment_percentile_ranks : {\n          values : {\n             10.0 : 49.865951742627345,\n             100.0 : 92.18395545473294,\n             1000.0 : 98.92761394101876,\n             10000.0 : 99.56773202397807,\n             1000000.0 : 100,\n             1.0E7 : 100\n         }\n      }\n   }\n}   As we can see, only 0.7% posts have more than 10k shares, onley 0.04% posts have more than 10k comment, but there is an odd here. 4.1% posts have more than 1M like (WHAT!!!). We can spot some strange here.", 
            "title": "Percentile Ranks Aggregation"
        }, 
        {
            "location": "/elasticsearch/#top-hits-aggregation", 
            "text": "Doc: Top hits Aggregation  Example  Request   Response  {", 
            "title": "Top hits Aggregation"
        }, 
        {
            "location": "/elasticsearch/#an-aggregation", 
            "text": "Doc: Link", 
            "title": "An Aggregation"
        }, 
        {
            "location": "/elasticsearch/#config", 
            "text": "elasticsearch.yml  discovery.zen.minimum_master_nodes: 1\ndiscovery.zen.ping.multicast.enabled: false\ndiscovery.zen.ping.unicast.hosts: [ localhost ]\n\nnetwork.host: 0.0.0.0\nhttp.cors.enabled: true\nhttp.cors.allow-origin: '*'\nscript.inline: on\nscript.indexed: on", 
            "title": "Config"
        }, 
        {
            "location": "/elasticsearch/#docker", 
            "text": "Image  https://hub.docker.com/r/_/elasticsearch/  Run  docker run -d -v  $PWD/esdata :/usr/share/elasticsearch/data elasticsearch  Docker Folder  elasticsearch/\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 elasticsearch.yml\n\u2514\u2500\u2500 Dockerfile  Dockerfile  FROM elasticsearch:2.2.0\n\nADD config/elasticsearch.yml /elasticsearch/config/elasticsearch.yml  Compose  elasticsearch:\n    build: ./elasticsearch/.\n    ports:\n       - 9200:9200\n       - 9300:9300\n    volumes:\n       - ./data/elasticsearch:/usr/share/elasticsearch/data", 
            "title": "Docker"
        }, 
        {
            "location": "/elasticsearch/#elasticsearch-search-ignore-accents", 
            "text": "The ICU  1   2  analysis plug-in for Elasticsearch uses the International Components for Unicode (ICU) libraries to provide a rich set of tools for dealing with Unicode. These include the icu_tokenizer, which is particularly useful for Asian languages, and a number of token filters that are essential for correct matching and sorting in all languages other than English.", 
            "title": "Elasticsearch: Search Ignore Accents"
        }, 
        {
            "location": "/elasticsearch/#step-1-install-icu-plugin-3", 
            "text": "cd /usr/share/elasticsearch\nsudo bin/plugin install analysis-icu", 
            "title": "Step 1: Install ICU-Plugin 3"
        }, 
        {
            "location": "/elasticsearch/#step-2-create-an-analyzer-setting", 
            "text": "settings : {\n       analysis : {\n          analyzer : {\n             vnanalysis : {\n                tokenizer :  icu_tokenizer ,\n                filter : [\n                   icu_folding ,\n                   icu_normalizer \n               ]\n            }\n         }\n      }\n   }", 
            "title": "Step 2: Create an analyzer setting:"
        }, 
        {
            "location": "/elasticsearch/#step-3-create-your-index-create-a-field-with-type-string-and-analyzer-is-vnanalysis-you-have-created", 
            "text": "key : {\n      type :  string ,\n      analyzer :  vnanalysis \n}", 
            "title": "Step 3: Create your index, create a field with type string and analyzer is vnanalysis you have created"
        }, 
        {
            "location": "/elasticsearch/#step-4-search-with-sense", 
            "text": "POST /your_index/your_doc_type/_search\n{\n    query : {\n       match : {\n          key :  kiem tra \n      }\n   }\n}", 
            "title": "Step 4: Search with sense"
        }, 
        {
            "location": "/elasticsearch/#es-import-csv-to-elasticsearch", 
            "text": "https://gist.github.com/clemsos/8668698", 
            "title": "ES: Import CSV to Elasticsearch"
        }, 
        {
            "location": "/elasticsearch/#install-lastest-elasticdump-with-nvm", 
            "text": "As a matter of best practice we\u2019ll update our packages:  apt-get update  The build-essential package should already be installed, however, we\u2019re going still going to include it in our command for installation:  apt-get install build-essential libssl-dev  To install or update nvm, you can use the install script using cURL:  curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bash  if you have below problem or after you type  nvm ls-remote  command it result N/A: curl: (77) error setting certificate verify locations:\n  CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none  head to this  1 :  or Wget:  wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bash  Don't forget to restart your terminal  Then you use the following command to list available versions of nodejs  nvm ls-remote  To download, compile, and install the latest v5.0.x release of node, do this:  nvm install 5.0  And then in any new shell just use the installed version:  nvm use 5.0  Or you can just run it:  nvm run 5.0 --version  Or, you can run any arbitrary command in a subshell with the desired version of node:  nvm exec 4.2 node --version  You can also get the path to the executable to where it was installed:  nvm which 5.0  Node Version Manager      how to solve https problem    ICU plug-in Github    Installing the ICU plug-in", 
            "title": "Install lastest Elasticdump with NVM"
        }
    ]
}