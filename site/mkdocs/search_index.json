{
    "docs": [
        {
            "location": "/", 
            "text": "Big Data Q\nA\n\n\n1. What is \"Big Data\"? \n1\n\n\nhttps://www.youtube.com/watch?v=TzxmjbL-i4Y\n\n\n2. How big is big data? \n2\n\n\n\n\n3. How much data is \"Big Data\"? \n3\n\n\n\n\n4. What are characteristics of \"Big Data\"? \n4\n\n\n\n\n5. What is big data ecosystem? \n5\n\n\n\n\n6. What is big data landscape \n6\n\n\n\n\n7. What are benefits of big data? \n7\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://www.youtube.com/watch?v=TzxmjbL-i4Y\n\n\n\n\n\n\nhttp://scoop.intel.com/what-happens-in-an-internet-minute/\n\n\n\n\n\n\nhttp://www.quora.com/How-much-data-is-Big-Data\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Big_data#Characteristics\n\n\n\n\n\n\nhttp://www.clearpeaks.com/blog/big-data/big-data-ecosystem-spark-and-tableau\n\n\n\n\n\n\nhttps://vladimerbotsvadze.wordpress.com/2015/01/28/the-big-data-landscape-technology-businessintelligence-analytics/\n\n\n\n\n\n\nhttp://blog.galaxyweblinks.com/big-data-with-bigger-benefits/", 
            "title": "Home"
        }, 
        {
            "location": "/#big-data-qa", 
            "text": "", 
            "title": "Big Data Q&amp;A"
        }, 
        {
            "location": "/#1-what-is-big-data-1", 
            "text": "https://www.youtube.com/watch?v=TzxmjbL-i4Y", 
            "title": "1. What is \"Big Data\"? 1"
        }, 
        {
            "location": "/#2-how-big-is-big-data-2", 
            "text": "", 
            "title": "2. How big is big data? 2"
        }, 
        {
            "location": "/#3-how-much-data-is-big-data-3", 
            "text": "", 
            "title": "3. How much data is \"Big Data\"? 3"
        }, 
        {
            "location": "/#4-what-are-characteristics-of-big-data-4", 
            "text": "", 
            "title": "4. What are characteristics of \"Big Data\"? 4"
        }, 
        {
            "location": "/#5-what-is-big-data-ecosystem-5", 
            "text": "", 
            "title": "5. What is big data ecosystem? 5"
        }, 
        {
            "location": "/#6-what-is-big-data-landscape-6", 
            "text": "", 
            "title": "6. What is big data landscape 6"
        }, 
        {
            "location": "/#7-what-are-benefits-of-big-data-7", 
            "text": "https://www.youtube.com/watch?v=TzxmjbL-i4Y    http://scoop.intel.com/what-happens-in-an-internet-minute/    http://www.quora.com/How-much-data-is-Big-Data    https://en.wikipedia.org/wiki/Big_data#Characteristics    http://www.clearpeaks.com/blog/big-data/big-data-ecosystem-spark-and-tableau    https://vladimerbotsvadze.wordpress.com/2015/01/28/the-big-data-landscape-technology-businessintelligence-analytics/    http://blog.galaxyweblinks.com/big-data-with-bigger-benefits/", 
            "title": "7. What are benefits of big data? 7"
        }, 
        {
            "location": "/hdfs/", 
            "text": "HDFS\n\n\n\n\n\n  The Hadoop Distributed File System (HDFS) \u2014 a subproject of the Apache Hadoop project\u2014is a distributed, highly fault-tolerant file system designed to run on low-cost commodity hardware. HDFS provides high-throughput access to application data and is suitable for applications with large data sets. This article explores the primary features of HDFS and provides a high-level view of the HDFS architecture.\n\n\n\n\n: \nsequenceiq/hadoop-docker\n\n\nBig Data Stack\n: \nHDFS\n, \nKibana\n, \nElasticSearch\n, \nNeo4J\n, \nApache Spark", 
            "title": "HDFS"
        }, 
        {
            "location": "/hdfs/#hdfs", 
            "text": "The Hadoop Distributed File System (HDFS) \u2014 a subproject of the Apache Hadoop project\u2014is a distributed, highly fault-tolerant file system designed to run on low-cost commodity hardware. HDFS provides high-throughput access to application data and is suitable for applications with large data sets. This article explores the primary features of HDFS and provides a high-level view of the HDFS architecture.  :  sequenceiq/hadoop-docker  Big Data Stack :  HDFS ,  Kibana ,  ElasticSearch ,  Neo4J ,  Apache Spark", 
            "title": "HDFS"
        }, 
        {
            "location": "/hbase/", 
            "text": "HBase\n\n\n\n  Apache HBase\u2122 is the Hadoop database, a distributed, scalable, big data store.  Download Apache HBase\u2122  Click here to download Apache HBase\u2122.\n\n\n\n\n\n\n1. When Would I Use Apache HBase? \n1\n\n\n\nHBase isn\u2019t suitable for every problem.\n\n\nFirst, make sure you have enough data. If you have hundreds of millions or billions of rows, then HBase is a good candidate. If you only have a few thousand/million rows, then using a traditional RDBMS might be a better choice due to the fact that all of your data might wind up on a single node (or two) and the rest of the cluster may be sitting idle.\n\n\nSecond, make sure you can live without all the extra features that an RDBMS provides (e.g., typed columns, secondary indexes, transactions, advanced query languages, etc.) An application built against an RDBMS cannot be \"ported\" to HBase by simply changing a JDBC driver, for example. Consider moving from an RDBMS to HBase as a complete redesign as opposed to a port.\n\n\nThird, make sure you have enough hardware. Even HDFS doesn\u2019t do well with anything less than 5 DataNodes (due to things such as HDFS block replication which has a default of 3), plus a NameNode.\n\n\nHBase can run quite well stand-alone on a laptop - but this should be considered a development configuration only.\n\n\n2. Features \n2\n\n\n\n\n\nLinear and modular scalability.\n\n\nStrictly consistent reads and writes.\n\n\nAutomatic and configurable sharding of tables\n\n\nAutomatic failover support between RegionServers.\n\n\nConvenient base classes for backing Hadoop MapReduce jobs with Apache HBase tables.\n\n\nEasy to use Java API for client access.\n\n\nBlock cache and Bloom Filters for real-time queries.\n\n\nQuery predicate push down via server side Filters\n\n\nThrift gateway and a REST-ful Web service that supports XML, Protobuf, and binary data encoding options\n\n\nExtensible jruby-based (JIRB) shell\n\n\nSupport for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX\n\n\n\n\n\n3. Architecture\n\n\n\n\n\nHBase Shell\n\n\n\n[code lang=\"shell\"]\n\n\nlist all table\n\n\nlist\n[/code]\n\n\nUp \n Running\n\n\n\n1. Download \n\n\n\nHBase 0.94.27 (HBase 0.98 won't work)\n\n\n[code lang=\"shell\"]\nwget https://www.apache.org/dist/hbase/hbase-0.94.27/hbase-0.94.27.tar.gz\ntar -xzf hbase-0.94.27.tar.gz\n[/code]\n\n\n2. Setup \n\n\n\n1.\n edit \n$HBASE_ROOT/conf/hbase-site.xml\n and add\n\n\n[code lang=\"xml\"]\n\n\n  \n\n    \nhbase.rootdir\n\n    \nfile:///full/path/to/where/the/data/should/be/stored\n\n  \n\n  \n\n    \nhbase.cluster.distributed\n\n    \nfalse\n\n  \n\n\n\n[/code]\n\n\n3. Verify \n\n\n\nGo to \nhttp://localhost:60010\n to see if HBase is running.\n\n\n\n\n\n\n\n\n\n\n\nWhen Should I Use HBase?\n\n\n\n\n\n\n\nHBase\n\n\n\n\n\n\n\n\n\n\nConfig HBase Remote\n\n\n1. Change \n/etc/hosts\n\n\n[code]\n127.0.0.1 [username]\n[server_ip] hbase.io\n[/code]\n\n\nExample\n\n\n[code]\n127.0.0.1 crawler\n192.168.0.151 hbase.io\n[/code]\n\n\n2. Change hostname\n\n\n[code]\nhostname hbase.io\n[/code]\n\n\n3. Change region servers\n\n\nEdit \n$HBASE_ROOT/conf/regionservers\n\n\n[code]\nhbase.io\n[/code]\n\n\n4. Change \n$HABSE_ROOT/conf/hbase-site.xml\n\n\n[code lang=\"xml\" title=\"hbase-site.xml\"]\n\n?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?\n\n\n\n\n\n\nhbase.rootdir\n\n\nfile:///home/username/Downloads/hbase/data\n\n\n\n\n\n\nhbase.cluster.distributed\n\n\nfalse\n\n\n\n\n\n\nhbase.zookeeper.quorum\n\n\nhbase.io\n\n\n\n\n\n\nzookeeper.znode.parent\n\n\n/hbase-unsecure\n\n\n\n\n\n\nhbase.rpc.timeout\n\n\n2592000000\n\n\n\n\n\n[/code]\n\n\nDocker\n\n\nHBase 0.94\n\n\nImage: https://github.com/Banno/docker-hbase-standalone\n\n\n[code]\ndocker run -d -p 2181:2181 -p 60000:60000 -p 60010:60010 -p 60020:60020 -p 60030:60030 banno/hbase-standalone\n[/code]\n\n\nCompose\n\n\n[code]\nhbase.vmware:\n    build: ./docker-hbase-standalone/.\n    command: \"/opt/hbase/hbase-0.94.15-cdh4.7.0/bin/hbase master start\"\n    hostname: hbase.vmware\n    ports:\n      - 2181:2181\n      - 60000:60000\n      - 60010:60010\n      - 60020:60020\n      - 60030:60030\n    volumes:\n      - ./docker-hbase-standalone/hbase-0.94.15-cdh4.7.0:/opt/hbase/hbase-0.94.15-cdh4.7.0\n      - ./data/hbase:/tmp/hbase-root/hbase\n/code]", 
            "title": "Apache HBase"
        }, 
        {
            "location": "/hbase/#hbase", 
            "text": "Apache HBase\u2122 is the Hadoop database, a distributed, scalable, big data store.  Download Apache HBase\u2122  Click here to download Apache HBase\u2122.", 
            "title": "HBase"
        }, 
        {
            "location": "/hbase/#list-all-table", 
            "text": "list\n[/code]", 
            "title": "list all table"
        }, 
        {
            "location": "/hbase/#config-hbase-remote", 
            "text": "", 
            "title": "Config HBase Remote"
        }, 
        {
            "location": "/hbase/#1-change-etchosts", 
            "text": "[code]\n127.0.0.1 [username]\n[server_ip] hbase.io\n[/code]  Example  [code]\n127.0.0.1 crawler\n192.168.0.151 hbase.io\n[/code]", 
            "title": "1. Change /etc/hosts"
        }, 
        {
            "location": "/hbase/#2-change-hostname", 
            "text": "[code]\nhostname hbase.io\n[/code]", 
            "title": "2. Change hostname"
        }, 
        {
            "location": "/hbase/#3-change-region-servers", 
            "text": "Edit  $HBASE_ROOT/conf/regionservers  [code]\nhbase.io\n[/code]", 
            "title": "3. Change region servers"
        }, 
        {
            "location": "/hbase/#4-change-habse_rootconfhbase-sitexml", 
            "text": "[code lang=\"xml\" title=\"hbase-site.xml\"] ?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?    hbase.rootdir  file:///home/username/Downloads/hbase/data    hbase.cluster.distributed  false    hbase.zookeeper.quorum  hbase.io    zookeeper.znode.parent  /hbase-unsecure    hbase.rpc.timeout  2592000000   \n[/code]", 
            "title": "4. Change $HABSE_ROOT/conf/hbase-site.xml"
        }, 
        {
            "location": "/hbase/#docker", 
            "text": "HBase 0.94  Image: https://github.com/Banno/docker-hbase-standalone  [code]\ndocker run -d -p 2181:2181 -p 60000:60000 -p 60010:60010 -p 60020:60020 -p 60030:60030 banno/hbase-standalone\n[/code]  Compose  [code]\nhbase.vmware:\n    build: ./docker-hbase-standalone/.\n    command: \"/opt/hbase/hbase-0.94.15-cdh4.7.0/bin/hbase master start\"\n    hostname: hbase.vmware\n    ports:\n      - 2181:2181\n      - 60000:60000\n      - 60010:60010\n      - 60020:60020\n      - 60030:60030\n    volumes:\n      - ./docker-hbase-standalone/hbase-0.94.15-cdh4.7.0:/opt/hbase/hbase-0.94.15-cdh4.7.0\n      - ./data/hbase:/tmp/hbase-root/hbase\n/code]", 
            "title": "Docker"
        }, 
        {
            "location": "/spark/", 
            "text": "Apache Spark\n\n\n\n\n\n  Apache Spark is an open-source \ncluster computing framework\n originally developed in the AMPLab at UC Berkeley. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's in-memory primitives provide performance up to 100 times faster for certain applications. By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms.\n\n\n\n\nInstallation\n\n\n\nRequirements: Hadoop, YARN\n\n\nInstall Hadoop\n\n\nInsatll YARN\n\n\nInstall Java\n\n\nVerification\n\n\n\nTutorial\n\n\n\nFrom Pandas to Apache Spark\u2019s DataFrame\n\n\nBig Data Stack\n: \nHDFS\n, \nKibana\n, \nElasticSearch\n, \nNeo4J\n, \nApache Spark\n\n\nApache Spark: Tutorials\n\n\nBeginners Guide: Apache Spark Machine Learning with Large Data\n\n\n\n\nSpark and Spark Streaming Unit Testing\n\n\nRecipes for Running Spark Streaming Applications in Production- Databricks\n\n\nSpark Streaming\n\n\n\n\nSpark and Spark Streaming Unit Testing\n\n\nRecipes for Running Spark Streaming Applications in Production- Databricks", 
            "title": "Apache Spark"
        }, 
        {
            "location": "/spark/#apache-spark", 
            "text": "Apache Spark is an open-source  cluster computing framework  originally developed in the AMPLab at UC Berkeley. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's in-memory primitives provide performance up to 100 times faster for certain applications. By allowing user programs to load data into a cluster's memory and query it repeatedly, Spark is well-suited to machine learning algorithms.", 
            "title": "Apache Spark"
        }, 
        {
            "location": "/spark/#apache-spark-tutorials", 
            "text": "Beginners Guide: Apache Spark Machine Learning with Large Data", 
            "title": "Apache Spark: Tutorials"
        }, 
        {
            "location": "/spark/#spark-streaming", 
            "text": "Spark and Spark Streaming Unit Testing  Recipes for Running Spark Streaming Applications in Production- Databricks", 
            "title": "Spark Streaming"
        }, 
        {
            "location": "/ambari/", 
            "text": "Ambari\n\n\nThe Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.\n\n\nAmbari enables System Administrators to:\n\n\n\n\n\n\nProvision a Hadoop Cluster\n\n\n\n\nAmbari provides a step-by-step wizard for installing Hadoop services across any number of hosts.\n\n\nAmbari handles configuration of Hadoop services for the cluster.\n\n\n\n\n\n\n\n\nManage a Hadoop Cluster\n\n\n\n\nAmbari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.\n\n\n\n\n\n\n\n\nMonitor a Hadoop Cluster\n\n\n\n\nAmbari provides a dashboard for monitoring health and status of the Hadoop cluster.\n\n\nAmbari leverages Ambari Metrics System for metrics collection.\n\n\nAmbari leverages Ambari Alert Framework for system alerting and will notify you when your attention is needed (e.g., a node goes down, remaining disk space is low, etc).\n\n\n\n\n\n\n\n\nAmbari enables Application Developers and System Integrators to:\n\n\n\n\nEasily integrate Hadoop provisioning, management, and monitoring capabilities to their own applications with the Ambari REST APIs.\n\n\n\n\nDocker\n\n\n\n\nReceipts:\n\n\n\n\nImage: \nsequenceiq/ambari\n (\ngit\n)\n\n\n\n\nMultinode cluster with Ambari 1.7.0 \n1\n\n\nGet the docker images\n\n\n[code]\ndocker pull sequenceiq/ambari:1.7.0\n[/code]\n\n\nGet ambari-functions\n[code]\ncurl -Lo .amb j.mp/docker-ambari-170 \n . .amb\n[/code]\n\n\nCreate your cluster \u2013 automated\n\n\n[code]\namb-deploy-cluster 3\n[/code]\n\n\n\n\n\n\n\n\n\n\nMultinode cluster with Ambari 1.7.0", 
            "title": "Apache Ambari"
        }, 
        {
            "location": "/ambari/#ambari", 
            "text": "The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.  Ambari enables System Administrators to:    Provision a Hadoop Cluster   Ambari provides a step-by-step wizard for installing Hadoop services across any number of hosts.  Ambari handles configuration of Hadoop services for the cluster.     Manage a Hadoop Cluster   Ambari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.     Monitor a Hadoop Cluster   Ambari provides a dashboard for monitoring health and status of the Hadoop cluster.  Ambari leverages Ambari Metrics System for metrics collection.  Ambari leverages Ambari Alert Framework for system alerting and will notify you when your attention is needed (e.g., a node goes down, remaining disk space is low, etc).     Ambari enables Application Developers and System Integrators to:   Easily integrate Hadoop provisioning, management, and monitoring capabilities to their own applications with the Ambari REST APIs.", 
            "title": "Ambari"
        }, 
        {
            "location": "/ambari/#docker", 
            "text": "Receipts:   Image:  sequenceiq/ambari  ( git )", 
            "title": "Docker"
        }, 
        {
            "location": "/ambari/#multinode-cluster-with-ambari-170-1", 
            "text": "Get the docker images  [code]\ndocker pull sequenceiq/ambari:1.7.0\n[/code]  Get ambari-functions\n[code]\ncurl -Lo .amb j.mp/docker-ambari-170   . .amb\n[/code]  Create your cluster \u2013 automated  [code]\namb-deploy-cluster 3\n[/code]      Multinode cluster with Ambari 1.7.0", 
            "title": "Multinode cluster with Ambari 1.7.0 1"
        }, 
        {
            "location": "/kibana/", 
            "text": "Kibana\n\n\n\n\n\n  Kibana is an \nopen source data visualization\n plugin for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data.", 
            "title": "Kibana"
        }, 
        {
            "location": "/kibana/#kibana", 
            "text": "Kibana is an  open source data visualization  plugin for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data.", 
            "title": "Kibana"
        }, 
        {
            "location": "/logstash/", 
            "text": "Logstash\n\n\nhttps://www.digitalocean.com/community/tutorials/how-to-use-logstash-and-kibana-to-centralize-logs-on-centos-6", 
            "title": "Logstash"
        }, 
        {
            "location": "/logstash/#logstash", 
            "text": "https://www.digitalocean.com/community/tutorials/how-to-use-logstash-and-kibana-to-centralize-logs-on-centos-6", 
            "title": "Logstash"
        }, 
        {
            "location": "/elasticsearch/", 
            "text": "Elasticsearch\n\n\n\n\n\n  Elasticsearch is a \nsearch server\n based on Lucene. It provides a distributed, multitenant-capable full-text search engine with a RESTful web interface and schema-free JSON documents. Elasticsearch is developed in Java and is released as open source under the terms of the Apache License. Elasticsearch is the second most popular enterprise search engine\n\n\n\n\n1. Basic Concenpts\n\n\n\n\n\n\n\n\n\nRelational Database\n\n\nElasticsearch\n\n\n\n\n\n\n\n\n\n\n\nDatabase\n\n\nIndex\n\n\n\n\n\n\n\nTable\n\n\nType\n\n\n\n\n\n\n\nRow\n\n\nDocument\n\n\n\n\n\n\n\nColumn\n\n\nField\n\n\n\n\n\n\n\nSchema\n\n\nMapping\n\n\n\n\n\n\n\n\n\n2. Index \n Query\n\n\n\nGet all indices\n\n\n\n\n  /_stats\n\n\n\n\nSearch API \n1\n\n\n\nSearch All\n\n\n\n\n  /bank/_search?q=*\n\n\n\n\nhits.hits\n \u2013 actual array of search results (defaults to first 10 documents)\n\n\nQuery Language\n\n\n\nelasticsearch provides a full \nQuery DSL\n based on JSON to define queries.\n\n\n\n  curl -XPOST /bank/_search\n\n\n\n\n// match all, limit 10 offset 10\n{\n  \nquery\n: { \nmatch_all\n: {} },\n  \nfrom\n: 10,\n  \nsize\n: 10\n}\n\n// select fields\n{\n  \nquery\n: { \nmatch_all\n: {} },\n  _source: [\naccount_number\n, \nbalance\n]\n  \nsize\n: 10\n}\n\n// where account equals 20\n{\n  \nquery\n: { \nmatch\n: { \naccount_number\n: 20 } }\n}\n\n\n\n\nFilter\n\n\ncurl -XPOST elastic:9200/index/type/_search -d '\n{\n  \nquery\n : {\n    \nfiltered\n :\n    {\n      \nquery\n : { \nterm\n : { \nfeature\n : 1 } } ,\n      \nfilter\n : {\n        \nand\n : [\n          {\n            \nrange\n: {\n              \n_timestamp\n: {\n                \nfrom\n: 1441964671000,\n                \nto\n: 1441964672000\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n\n\n\n\nSort\n\n\ncurl -XPOST elastic:9200/index/type/_search -d '\n{\n  \nquery\n : {\n    \nfiltered\n :\n    {\n      \nquery\n : { \nterm\n : { \nfeature\n : 1 } } ,\n      \nfilter\n : {\n        \nand\n : [\n          {\n            \nrange\n: {\n              \n_timestamp\n: {\n                \nfrom\n: 1441964671000,\n                \nto\n: 1441964672000\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n\n\n\n\n3. Mapping\n\n\n\nTimestamp \n2\n\n\n\nEnable and store timestamp\n\n\n\n  curl -XPOST localhost:9200/test\n\n\n\n\n{\n\nmappings\n : {\n    \n_default_\n:{\n        \n_timestamp\n : {\n            \nenabled\n : true,\n            \nstore\n : true\n        }\n    }\n  }\n}'\n\n\n\n\nRelationships Management \n3\n \n4\n\n\n\nInner Object\n\n\n\n\n\ud83d\udc4d Easy, fast, performant\n\n\n\ud83d\udc4e No need for special queries\n\n\n\u261b Only applicable when one-to-one relationships are maintained\n\n\n\n\n\nNested\n\n\n\n\n\ud83d\udc4d Nested docs are stored in the same Lucene block as each other, which helps read/query  performance. Reading a nested doc is faster than the equivalent parent/child.\n\n\n\ud83d\udc4e Updating a single field in a nested document (parent or nested children) forces ES to reindex the entire nested document. This can be very expensive for large nested docs\n\n\n\ud83d\udc4e \u201cCross referencing\u201d nested documents is impossible\n\n\n\u261b Best suited for data that does not change frequently\n\n\n\n\n\nParent/Child\n\n\n\n\n\ud83d\udc4d Updating a child doc does not affect the parent or any other children, which can potentially save a lot of indexing on large docs\n\n\n\ud83d\udc4e Children are stored separately from the parent, but are routed to the same shard. So parent/children are slightly less performance on read/query than nested\n\n\n\ud83d\udc4e Parent/child mappings have a bit extra memory overhead, since ES maintains a \u201cjoin\u201d list in memory\n\n\n\ud83d\udc4e Sorting/scoring can be difficult with Parent/Child since the Has Child/Has Parent operations can be opaque at times\n\n\n\n\n\nDenormalization\n\n\n\n\n\ud83d\udc4d You get to manage all the relations yourself!\n\n\n\ud83d\udc4e Most flexible, most administrative overhead\n\n\n\u261b May be more or less performant depending on your setup\n\n\n\n\n\n4. Backup\n\n\n\nElastic Dump \n5\n\n\n\nTools for moving and saving indicies.\n\n\nbin/elasticdump\n  --input=http://localhost:9200/index_1\n  --output=http://localhost:9200/index_1_backup\n  --type=data\n  --scrollTime=100\n\n\n\n\nAlias \n6\n\n\n\ncurl -XPOST 'http://localhost:9200/_aliases' -d '\n{\n    \nquot;actions\nquot; : [\n        { \nquot;remove\nquot; : { \nquot;index\nquot; : \nquot;test1\nquot;, \nquot;alias\nquot; : \nquot;alias1\nquot; } },\n        { \nquot;add\nquot; : { \nquot;index\nquot; : \nquot;test1\nquot;, \nquot;alias\nquot; : \nquot;alias2\nquot; } }\n    ]\n}'\n\n\n\n\n5. Module Scripting \n7\n\n\n\nRanking\n\n\n\nRank #2 from DB-Engines Ranking of Search Engines\n\n\n\n\n\n\n\n\n\n\n\nThe Search API\n\n\n\n\n\n\nhttp://stackoverflow.com/a/17146144/772391\n\n\n\n\n\n\n\nhttp://stackoverflow.com/a/23407367/772391\n\n\n\n\n\n\n\nhttps://www.elastic.co/guide/en/elasticsearch/guide/current/modeling-your-data.html\n\n\n\n\n\n\nhttps://github.com/taskrabbit/elasticsearch-dump\n\n\n\n\n\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html\n\n\n\n\n\n\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html\n\n\n\n\n\n\n\n\n\n\nElasticsearch tutorial series 1: Metric Aggregations with Social Network Data\n\n\nTable of content\n\n\n\n\nAvg, Max, Min, Sum Aggregation\n\n\nCardinality Aggregation\n\n\nStats Aggregation\n\n\nExtended Stats Aggregation\n\n\nPercentile Aggregation\n\n\nPercentile Ranks Aggregation\n\n\nTop hits Aggregation\n\n\n\n\nAvg, Max, Min, Sum, Count Aggregation\n\n\nDoc: Avg Aggregation\n, \nDoc: Max Aggregation\n, \nDoc: Min Aggregation\n\n\nGet max, min, avg, sum, count about number of likes, shares, comments\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\naggs\n:{\nsum_like\n:{\nsum\n:{\nfield\n:\nnum_like\n}},\nmin_like\n:{\nmin\n:{\nfield\n:\nnum_like\n}},\navg_like\n:{\navg\n:{\nfield\n:\nnum_like\n}},\nmax_like\n:{\nmax\n:{\nfield\n:\nnum_like\n}},\nsum_share\n:{\nsum\n:{\nfield\n:\nnum_share\n}},\nmin_share\n:{\nmin\n:{\nfield\n:\nnum_share\n}},\navg_share\n:{\navg\n:{\nfield\n:\nnum_share\n}},\nmax_share\n:{\nmax\n:{\nfield\n:\nnum_share\n}},\nsum_comment\n:{\nsum\n:{\nfield\n:\nnum_comment\n}},\nmin_comment\n:{\nmin\n:{\nfield\n:\nnum_comment\n}},\navg_comment\n:{\navg\n:{\nfield\n:\nnum_comment\n}},\nmax_comment\n:{\nmax\n:{\nfield\n:\nnum_comment\n}}}}\n\n\n\n\nRequest\n\n\n{\n\naggregations\n: {\n      \navg_comment\n: {\n         \nvalue\n: 75.23860589812332\n      },\n      \nmin_like\n: {\n         \nvalue\n: 0\n      },\n      \navg_like\n: {\n         \nvalue\n: 1761974365266098.2\n      },\n      \nsum_like\n: {\n         \nvalue\n: 3238508883359088600\n      },\n      \nmax_share\n: {\n         \nvalue\n: 30407\n      },\n      \nmax_comment\n: {\n         \nvalue\n: 11000\n      },\n      \nsum_share\n: {\n         \nvalue\n: 117844\n      },\n      \nmax_like\n: {\n         \nvalue\n: 2751488761761411000\n      },\n      \navg_share\n: {\n         \nvalue\n: 250.19957537154988\n      },\n      \nsum_comment\n: {\n         \nvalue\n: 28064\n      },\n      \nmin_comment\n: {\n         \nvalue\n: 2\n      },\n      \nmin_share\n: {\n         \nvalue\n: 1\n      }\n   }\n}\n\n\n\n\nCardinality Aggregation\n\n\nCardinality Aggregation\n\n\nGet total of users\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\n    \naggs\n : {\n        \nnum_authors\n : { \ncardinality\n : { \nfield\n : \nfrom.fb_id\n } }\n    }\n}\n\n\n\n\nResponse\n\n\n{\n   \naggregations\n: {\n      \nnum_authors\n: {\n         \nvalue\n: 7385\n      }\n   }\n}\n\n\n\n\nStats Aggregation\n\n\nDoc: Stats Aggregation\n\n\nBasic Stats of like, share \n comment\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\n    \naggs\n : {\n        \nshares\n : { \nstats\n : { \nfield\n : \nnum_share\n } },\n        \nlikes\n : { \nstats\n : { \nfield\n : \nnum_like\n } },\n        \ncomments\n : { \nstats\n : { \nfield\n : \nnum_comment\n } }\n    }\n}\n\n\n\n\nResponse\n\n\n{\n   \naggregations\n: {\n      \nshares\n: {\n         \ncount\n: 471,\n         \nmin\n: 1,\n         \nmax\n: 30407,\n         \navg\n: 250.19957537154988,\n         \nsum\n: 117844\n      },\n      \ncomments\n: {\n         \ncount\n: 373,\n         \nmin\n: 2,\n         \nmax\n: 11000,\n         \navg\n: 75.23860589812332,\n         \nsum\n: 28064\n      },\n      \nlikes\n: {\n         \ncount\n: 1838,\n         \nmin\n: 0,\n         \nmax\n: 2751488761761411000,\n         \navg\n: 1761974365266098.2,\n         \nsum\n: 3238508883359088600\n      }\n   }\n}\n\n\n\n\nExtended Stats Aggregation\n\n\nExtended Stats Aggregation\n\n\nStats of like, share \n comment with more metrics, such as sum, std_deviation, std_deviation_bounds, variance\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\n    \naggs\n : {\n        \nlike_stats\n : { \nextended_stats\n : { \nfield\n : \nnum_like\n } },\n        \nshare_stats\n : { \nextended_stats\n : { \nfield\n : \nnum_share\n } },\n        \ncomment_stats\n : { \nextended_stats\n : { \nfield\n : \nnum_comment\n } }\n    }\n}\n\n\n\n\nResponse\n\n\n{\n   \naggregations\n: {\n      \nlike_stats\n: {\n         \ncount\n: 1838,\n         \nmin\n: 0,\n         \nmax\n: 2751488761761411000,\n         \navg\n: 1761974365266098.2,\n         \nsum\n: 3238508883359088600,\n         \nsum_of_squares\n: 7.667542671405507e+36,\n         \nvariance\n: 4.168572634260795e+33,\n         \nstd_deviation\n: 64564484310345070,\n         \nstd_deviation_bounds\n: {\n            \nupper\n: 130890942985956240,\n            \nlower\n: -127366994255424050\n         }\n      },\n      \nshare_stats\n: {\n         \ncount\n: 471,\n         \nmin\n: 1,\n         \nmax\n: 30407,\n         \navg\n: 250.19957537154988,\n         \nsum\n: 117844,\n         \nsum_of_squares\n: 1769467022,\n         \nvariance\n: 3694230.367812983,\n         \nstd_deviation\n: 1922.0380765773043,\n         \nstd_deviation_bounds\n: {\n            \nupper\n: 4094.2757285261587,\n            \nlower\n: -3593.8765777830586\n         }\n      },\n      \ncomment_stats\n: {\n         \ncount\n: 373,\n         \nmin\n: 2,\n         \nmax\n: 11000,\n         \navg\n: 75.23860589812332,\n         \nsum\n: 28064,\n         \nsum_of_squares\n: 131531392,\n         \nvariance\n: 346970.2299304962,\n         \nstd_deviation\n: 589.0417896299856,\n         \nstd_deviation_bounds\n: {\n            \nupper\n: 1253.3221851580945,\n            \nlower\n: -1102.844973361848\n         }\n      }\n   }\n}\n\n\n\n\nPercentiles Aggregation\n\n\nDoc: Percentiles Aggregation\n\n\nComment, Like, Share Percentiles\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\naggs\n:{\nlike_percentiles\n:{\npercentiles\n:{\nfield\n:\nnum_like\n}},\nshare_percentiles\n:{\npercentiles\n:{\nfield\n:\nnum_share\n}},\ncomment_percentiles\n:{\npercentiles\n:{\nfield\n:\nnum_comment\n}}}}\n\n\n\n\nResponse\n\n\n{\n\naggregations\n: {\n      \nlike_percentiles\n: {\n         \nvalues\n: {\n            \n1.0\n: 0,\n            \n5.0\n: 0,\n            \n25.0\n: 4,\n            \n50.0\n: 18.35,\n            \n75.0\n: 72.53579545454545,\n            \n95.0\n: 71343.74999999999,\n            \n99.0\n: 4338260523723.276\n         }\n      },\n      \ncomment_percentiles\n: {\n         \nvalues\n: {\n            \n1.0\n: 2,\n            \n5.0\n: 2,\n            \n25.0\n: 5,\n            \n50.0\n: 10,\n            \n75.0\n: 26,\n            \n95.0\n: 139.39999999999998,\n            \n99.0\n: 1000\n         }\n      },\n      \nshare_percentiles\n: {\n         \nvalues\n: {\n            \n1.0\n: 1,\n            \n5.0\n: 1,\n            \n25.0\n: 1,\n            \n50.0\n: 4,\n            \n75.0\n: 25,\n            \n95.0\n: 251.5,\n            \n99.0\n: 5560.3\n         }\n      }\n   }\n}\n\n\n\n\nLike Percentiles with custom percents\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\n   \naggs\n: {\n      \nshare_percentiles\n: {\n         \npercentiles\n: {\n            \nfield\n: \nnum_share\n,\n            \npercents\n: [0, 10, 80, 90, 95]\n         }\n      }\n   }\n}\n\n\n\n\nResponse\n\n\n{\n   \naggregations\n: {\n      \nshare_percentiles\n: {\n         \nvalues\n: {\n            \n0.0\n: 1,\n            \n10.0\n: 1,\n            \n80.0\n: 37.33333333333333,\n            \n90.0\n: 97,\n            \n95.0\n: 251.5\n         }\n      }\n   }\n}\n\n\n\n\nPercentile Ranks Aggregation\n\n\nDoc: Percentile Ranks Aggregation\n\n\nHow like, share, comment distribute\n\n\nRequest\n\n\nPOST /facebook_crawler/post/_search\n{\n   \naggs\n: {\n      \nlike_percentile_ranks\n: {\n         \npercentile_ranks\n: {\n            \nfield\n: \nnum_like\n,\n            \nvalues\n: [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      },\n      \nshare_percentile_ranks\n: {\n         \npercentile_ranks\n: {\n            \nfield\n: \nnum_share\n,\n            \nvalues\n: [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      },\n      \ncomment_percentile_ranks\n: {\n         \npercentile_ranks\n: {\n            \nfield\n: \nnum_comment\n,\n            \nvalues\n: [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      }\n   }\n}\n\n\n\n\nResponse\n\n\n{\n   \naggregations\n: {\n      \nshare_percentile_ranks\n: {\n         \nvalues\n: {\n            \n10.0\n: 60.438782731776364,\n            \n100.0\n: 89.91507430997878,\n            \n1000.0\n: 97.37406386327386,\n            \n10000.0\n: 99.31579836222765,\n            \n1000000.0\n: 100,\n            \n1.0E7\n: 100\n         }\n      },\n      \nlike_percentile_ranks\n: {\n         \nvalues\n: {\n            \n10.0\n: 39.281828073993466,\n            \n100.0\n: 79.39530545624125,\n            \n1000.0\n: 90.98349676683587,\n            \n10000.0\n: 94.14527905373414,\n            \n1000000.0\n: 95.9014681663581,\n            \n1.0E7\n: 96.57661015941164\n         }\n      },\n      \ncomment_percentile_ranks\n: {\n         \nvalues\n: {\n            \n10.0\n: 49.865951742627345,\n            \n100.0\n: 92.18395545473294,\n            \n1000.0\n: 98.92761394101876,\n            \n10000.0\n: 99.56773202397807,\n            \n1000000.0\n: 100,\n            \n1.0E7\n: 100\n         }\n      }\n   }\n}\n\n\n\n\n\n\nAs we can see, only 0.7% posts have more than 10k shares, onley 0.04% posts have more than 10k comment, but there is an odd here. 4.1% posts have more than 1M like (WHAT!!!). We can spot some strange here.\n\n\n\n\nTop hits Aggregation\n\n\nDoc: Top hits Aggregation\n\n\nExample\n\n\nRequest\n\n\n\n\n\nResponse\n\n\n{\n\n\n\n\n\nAn Aggregation\n\n\nDoc: Link\n\n\nConfig\n\n\nelasticsearch.yml\n\n\ndiscovery.zen.minimum_master_nodes: 1\ndiscovery.zen.ping.multicast.enabled: false\ndiscovery.zen.ping.unicast.hosts: [\nlocalhost\n]\n\nnetwork.host: 0.0.0.0\nhttp.cors.enabled: true\nhttp.cors.allow-origin: '*'\nscript.inline: on\nscript.indexed: on\n\n\n\n\nDocker\n\n\nImage\n\n\nhttps://hub.docker.com/r/_/elasticsearch/\n\n\nRun\n\n\ndocker run -d -v \n$PWD/esdata\n:/usr/share/elasticsearch/data elasticsearch\n\n\n\n\nDocker Folder\n\n\nelasticsearch/\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 elasticsearch.yml\n\u2514\u2500\u2500 Dockerfile\n\n\n\n\nDockerfile\n\n\nFROM elasticsearch:2.2.0\n\nADD config/elasticsearch.yml /elasticsearch/config/elasticsearch.yml\n\n\n\n\nCompose\n\n\nelasticsearch:\n    build: ./elasticsearch/.\n    ports:\n       - 9200:9200\n       - 9300:9300\n    volumes:\n       - ./data/elasticsearch:/usr/share/elasticsearch/data\n\n\n\n\nElasticsearch: Search Ignore Accents\n\n\nThe ICU \n1\n \n2\n analysis plug-in for Elasticsearch uses the International Components for Unicode (ICU) libraries to provide a rich set of tools for dealing with Unicode. These include the icu_tokenizer, which is particularly useful for Asian languages, and a number of token filters that are essential for correct matching and sorting in all languages other than English.\n\n\nStep 1: Install ICU-Plugin \n3\n\n\ncd /usr/share/elasticsearch\nsudo bin/plugin install analysis-icu\n\n\n\n\nStep 2: Create an analyzer setting:\n\n\nsettings\n: {\n      \nanalysis\n: {\n         \nanalyzer\n: {\n            \nvnanalysis\n: {\n               \ntokenizer\n: \nicu_tokenizer\n,\n               \nfilter\n: [\n                  \nicu_folding\n,\n                  \nicu_normalizer\n\n               ]\n            }\n         }\n      }\n   }\n\n\n\n\nStep 3: Create your index, create a field with type string and analyzer is \nvnanalysis\n you have created\n\n\nkey\n: {\n     \ntype\n: \nstring\n,\n     \nanalyzer\n: \nvnanalysis\n\n}\n\n\n\n\nStep 4: Search with \nsense\n\n\nPOST /your_index/your_doc_type/_search\n{\n   \nquery\n: {\n      \nmatch\n: {\n         \nkey\n: \nkiem tra\n\n      }\n   }\n}\n\n\n\n\nES: Import CSV to Elasticsearch\n\n\nhttps://gist.github.com/clemsos/8668698\n\n\nInstall lastest Elasticdump with NVM\n\n\nAs a matter of best practice we\u2019ll update our packages:\n\n\napt-get update\n\n\n\n\nThe build-essential package should already be installed, however, we\u2019re going still going to include it in our command for installation:\n\n\napt-get install build-essential libssl-dev\n\n\n\n\nTo install or update nvm, you can use the install script using cURL:\n\n\ncurl -o- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bash\n\n\n\n\nif you have below problem or after you type \nnvm ls-remote\n command it result N/A:\n\ncurl: (77) error setting certificate verify locations:\n  CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n\n\nhead to this \n1\n:\n\n\nor Wget:\n\n\nwget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bash\n\n\n\n\nDon't forget to restart your terminal\n\n\nThen you use the following command to list available versions of nodejs\n\n\nnvm ls-remote\n\n\n\n\nTo download, compile, and install the latest v5.0.x release of node, do this:\n\n\nnvm install 5.0\n\n\n\n\nAnd then in any new shell just use the installed version:\n\n\nnvm use 5.0\n\n\n\n\nOr you can just run it:\n\n\nnvm run 5.0 --version\n\n\n\n\nOr, you can run any arbitrary command in a subshell with the desired version of node:\n\n\nnvm exec 4.2 node --version\n\n\n\n\nYou can also get the path to the executable to where it was installed:\n\n\nnvm which 5.0\n\n\n\n\nNode Version Manager\n\n\n\n\n\n\n\n\n\n\nhow to solve https problem\n\n\n\n\n\n\nICU plug-in Github\n\n\n\n\n\n\nInstalling the ICU plug-in", 
            "title": "Elasticsearch"
        }, 
        {
            "location": "/elasticsearch/#elasticsearch", 
            "text": "Elasticsearch is a  search server  based on Lucene. It provides a distributed, multitenant-capable full-text search engine with a RESTful web interface and schema-free JSON documents. Elasticsearch is developed in Java and is released as open source under the terms of the Apache License. Elasticsearch is the second most popular enterprise search engine", 
            "title": "Elasticsearch"
        }, 
        {
            "location": "/elasticsearch/#elasticsearch-tutorial-series-1-metric-aggregations-with-social-network-data", 
            "text": "Table of content   Avg, Max, Min, Sum Aggregation  Cardinality Aggregation  Stats Aggregation  Extended Stats Aggregation  Percentile Aggregation  Percentile Ranks Aggregation  Top hits Aggregation", 
            "title": "Elasticsearch tutorial series 1: Metric Aggregations with Social Network Data"
        }, 
        {
            "location": "/elasticsearch/#avg-max-min-sum-count-aggregation", 
            "text": "Doc: Avg Aggregation ,  Doc: Max Aggregation ,  Doc: Min Aggregation  Get max, min, avg, sum, count about number of likes, shares, comments  Request  POST /facebook_crawler/post/_search\n{ aggs :{ sum_like :{ sum :{ field : num_like }}, min_like :{ min :{ field : num_like }}, avg_like :{ avg :{ field : num_like }}, max_like :{ max :{ field : num_like }}, sum_share :{ sum :{ field : num_share }}, min_share :{ min :{ field : num_share }}, avg_share :{ avg :{ field : num_share }}, max_share :{ max :{ field : num_share }}, sum_comment :{ sum :{ field : num_comment }}, min_comment :{ min :{ field : num_comment }}, avg_comment :{ avg :{ field : num_comment }}, max_comment :{ max :{ field : num_comment }}}}  Request  { aggregations : {\n       avg_comment : {\n          value : 75.23860589812332\n      },\n       min_like : {\n          value : 0\n      },\n       avg_like : {\n          value : 1761974365266098.2\n      },\n       sum_like : {\n          value : 3238508883359088600\n      },\n       max_share : {\n          value : 30407\n      },\n       max_comment : {\n          value : 11000\n      },\n       sum_share : {\n          value : 117844\n      },\n       max_like : {\n          value : 2751488761761411000\n      },\n       avg_share : {\n          value : 250.19957537154988\n      },\n       sum_comment : {\n          value : 28064\n      },\n       min_comment : {\n          value : 2\n      },\n       min_share : {\n          value : 1\n      }\n   }\n}", 
            "title": "Avg, Max, Min, Sum, Count Aggregation"
        }, 
        {
            "location": "/elasticsearch/#cardinality-aggregation", 
            "text": "Cardinality Aggregation  Get total of users  Request  POST /facebook_crawler/post/_search\n{\n     aggs  : {\n         num_authors  : {  cardinality  : {  field  :  from.fb_id  } }\n    }\n}  Response  {\n    aggregations : {\n       num_authors : {\n          value : 7385\n      }\n   }\n}", 
            "title": "Cardinality Aggregation"
        }, 
        {
            "location": "/elasticsearch/#stats-aggregation", 
            "text": "Doc: Stats Aggregation  Basic Stats of like, share   comment  Request  POST /facebook_crawler/post/_search\n{\n     aggs  : {\n         shares  : {  stats  : {  field  :  num_share  } },\n         likes  : {  stats  : {  field  :  num_like  } },\n         comments  : {  stats  : {  field  :  num_comment  } }\n    }\n}  Response  {\n    aggregations : {\n       shares : {\n          count : 471,\n          min : 1,\n          max : 30407,\n          avg : 250.19957537154988,\n          sum : 117844\n      },\n       comments : {\n          count : 373,\n          min : 2,\n          max : 11000,\n          avg : 75.23860589812332,\n          sum : 28064\n      },\n       likes : {\n          count : 1838,\n          min : 0,\n          max : 2751488761761411000,\n          avg : 1761974365266098.2,\n          sum : 3238508883359088600\n      }\n   }\n}", 
            "title": "Stats Aggregation"
        }, 
        {
            "location": "/elasticsearch/#extended-stats-aggregation", 
            "text": "Extended Stats Aggregation  Stats of like, share   comment with more metrics, such as sum, std_deviation, std_deviation_bounds, variance  Request  POST /facebook_crawler/post/_search\n{\n     aggs  : {\n         like_stats  : {  extended_stats  : {  field  :  num_like  } },\n         share_stats  : {  extended_stats  : {  field  :  num_share  } },\n         comment_stats  : {  extended_stats  : {  field  :  num_comment  } }\n    }\n}  Response  {\n    aggregations : {\n       like_stats : {\n          count : 1838,\n          min : 0,\n          max : 2751488761761411000,\n          avg : 1761974365266098.2,\n          sum : 3238508883359088600,\n          sum_of_squares : 7.667542671405507e+36,\n          variance : 4.168572634260795e+33,\n          std_deviation : 64564484310345070,\n          std_deviation_bounds : {\n             upper : 130890942985956240,\n             lower : -127366994255424050\n         }\n      },\n       share_stats : {\n          count : 471,\n          min : 1,\n          max : 30407,\n          avg : 250.19957537154988,\n          sum : 117844,\n          sum_of_squares : 1769467022,\n          variance : 3694230.367812983,\n          std_deviation : 1922.0380765773043,\n          std_deviation_bounds : {\n             upper : 4094.2757285261587,\n             lower : -3593.8765777830586\n         }\n      },\n       comment_stats : {\n          count : 373,\n          min : 2,\n          max : 11000,\n          avg : 75.23860589812332,\n          sum : 28064,\n          sum_of_squares : 131531392,\n          variance : 346970.2299304962,\n          std_deviation : 589.0417896299856,\n          std_deviation_bounds : {\n             upper : 1253.3221851580945,\n             lower : -1102.844973361848\n         }\n      }\n   }\n}", 
            "title": "Extended Stats Aggregation"
        }, 
        {
            "location": "/elasticsearch/#percentiles-aggregation", 
            "text": "Doc: Percentiles Aggregation  Comment, Like, Share Percentiles  Request  POST /facebook_crawler/post/_search\n{ aggs :{ like_percentiles :{ percentiles :{ field : num_like }}, share_percentiles :{ percentiles :{ field : num_share }}, comment_percentiles :{ percentiles :{ field : num_comment }}}}  Response  { aggregations : {\n       like_percentiles : {\n          values : {\n             1.0 : 0,\n             5.0 : 0,\n             25.0 : 4,\n             50.0 : 18.35,\n             75.0 : 72.53579545454545,\n             95.0 : 71343.74999999999,\n             99.0 : 4338260523723.276\n         }\n      },\n       comment_percentiles : {\n          values : {\n             1.0 : 2,\n             5.0 : 2,\n             25.0 : 5,\n             50.0 : 10,\n             75.0 : 26,\n             95.0 : 139.39999999999998,\n             99.0 : 1000\n         }\n      },\n       share_percentiles : {\n          values : {\n             1.0 : 1,\n             5.0 : 1,\n             25.0 : 1,\n             50.0 : 4,\n             75.0 : 25,\n             95.0 : 251.5,\n             99.0 : 5560.3\n         }\n      }\n   }\n}  Like Percentiles with custom percents  Request  POST /facebook_crawler/post/_search\n{\n    aggs : {\n       share_percentiles : {\n          percentiles : {\n             field :  num_share ,\n             percents : [0, 10, 80, 90, 95]\n         }\n      }\n   }\n}  Response  {\n    aggregations : {\n       share_percentiles : {\n          values : {\n             0.0 : 1,\n             10.0 : 1,\n             80.0 : 37.33333333333333,\n             90.0 : 97,\n             95.0 : 251.5\n         }\n      }\n   }\n}", 
            "title": "Percentiles Aggregation"
        }, 
        {
            "location": "/elasticsearch/#percentile-ranks-aggregation", 
            "text": "Doc: Percentile Ranks Aggregation  How like, share, comment distribute  Request  POST /facebook_crawler/post/_search\n{\n    aggs : {\n       like_percentile_ranks : {\n          percentile_ranks : {\n             field :  num_like ,\n             values : [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      },\n       share_percentile_ranks : {\n          percentile_ranks : {\n             field :  num_share ,\n             values : [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      },\n       comment_percentile_ranks : {\n          percentile_ranks : {\n             field :  num_comment ,\n             values : [10, 100, 1000, 10000, 1000000, 10000000]\n         }\n      }\n   }\n}  Response  {\n    aggregations : {\n       share_percentile_ranks : {\n          values : {\n             10.0 : 60.438782731776364,\n             100.0 : 89.91507430997878,\n             1000.0 : 97.37406386327386,\n             10000.0 : 99.31579836222765,\n             1000000.0 : 100,\n             1.0E7 : 100\n         }\n      },\n       like_percentile_ranks : {\n          values : {\n             10.0 : 39.281828073993466,\n             100.0 : 79.39530545624125,\n             1000.0 : 90.98349676683587,\n             10000.0 : 94.14527905373414,\n             1000000.0 : 95.9014681663581,\n             1.0E7 : 96.57661015941164\n         }\n      },\n       comment_percentile_ranks : {\n          values : {\n             10.0 : 49.865951742627345,\n             100.0 : 92.18395545473294,\n             1000.0 : 98.92761394101876,\n             10000.0 : 99.56773202397807,\n             1000000.0 : 100,\n             1.0E7 : 100\n         }\n      }\n   }\n}   As we can see, only 0.7% posts have more than 10k shares, onley 0.04% posts have more than 10k comment, but there is an odd here. 4.1% posts have more than 1M like (WHAT!!!). We can spot some strange here.", 
            "title": "Percentile Ranks Aggregation"
        }, 
        {
            "location": "/elasticsearch/#top-hits-aggregation", 
            "text": "Doc: Top hits Aggregation  Example  Request   Response  {", 
            "title": "Top hits Aggregation"
        }, 
        {
            "location": "/elasticsearch/#an-aggregation", 
            "text": "Doc: Link", 
            "title": "An Aggregation"
        }, 
        {
            "location": "/elasticsearch/#config", 
            "text": "elasticsearch.yml  discovery.zen.minimum_master_nodes: 1\ndiscovery.zen.ping.multicast.enabled: false\ndiscovery.zen.ping.unicast.hosts: [ localhost ]\n\nnetwork.host: 0.0.0.0\nhttp.cors.enabled: true\nhttp.cors.allow-origin: '*'\nscript.inline: on\nscript.indexed: on", 
            "title": "Config"
        }, 
        {
            "location": "/elasticsearch/#docker", 
            "text": "Image  https://hub.docker.com/r/_/elasticsearch/  Run  docker run -d -v  $PWD/esdata :/usr/share/elasticsearch/data elasticsearch  Docker Folder  elasticsearch/\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 elasticsearch.yml\n\u2514\u2500\u2500 Dockerfile  Dockerfile  FROM elasticsearch:2.2.0\n\nADD config/elasticsearch.yml /elasticsearch/config/elasticsearch.yml  Compose  elasticsearch:\n    build: ./elasticsearch/.\n    ports:\n       - 9200:9200\n       - 9300:9300\n    volumes:\n       - ./data/elasticsearch:/usr/share/elasticsearch/data", 
            "title": "Docker"
        }, 
        {
            "location": "/elasticsearch/#elasticsearch-search-ignore-accents", 
            "text": "The ICU  1   2  analysis plug-in for Elasticsearch uses the International Components for Unicode (ICU) libraries to provide a rich set of tools for dealing with Unicode. These include the icu_tokenizer, which is particularly useful for Asian languages, and a number of token filters that are essential for correct matching and sorting in all languages other than English.", 
            "title": "Elasticsearch: Search Ignore Accents"
        }, 
        {
            "location": "/elasticsearch/#step-1-install-icu-plugin-3", 
            "text": "cd /usr/share/elasticsearch\nsudo bin/plugin install analysis-icu", 
            "title": "Step 1: Install ICU-Plugin 3"
        }, 
        {
            "location": "/elasticsearch/#step-2-create-an-analyzer-setting", 
            "text": "settings : {\n       analysis : {\n          analyzer : {\n             vnanalysis : {\n                tokenizer :  icu_tokenizer ,\n                filter : [\n                   icu_folding ,\n                   icu_normalizer \n               ]\n            }\n         }\n      }\n   }", 
            "title": "Step 2: Create an analyzer setting:"
        }, 
        {
            "location": "/elasticsearch/#step-3-create-your-index-create-a-field-with-type-string-and-analyzer-is-vnanalysis-you-have-created", 
            "text": "key : {\n      type :  string ,\n      analyzer :  vnanalysis \n}", 
            "title": "Step 3: Create your index, create a field with type string and analyzer is vnanalysis you have created"
        }, 
        {
            "location": "/elasticsearch/#step-4-search-with-sense", 
            "text": "POST /your_index/your_doc_type/_search\n{\n    query : {\n       match : {\n          key :  kiem tra \n      }\n   }\n}", 
            "title": "Step 4: Search with sense"
        }, 
        {
            "location": "/elasticsearch/#es-import-csv-to-elasticsearch", 
            "text": "https://gist.github.com/clemsos/8668698", 
            "title": "ES: Import CSV to Elasticsearch"
        }, 
        {
            "location": "/elasticsearch/#install-lastest-elasticdump-with-nvm", 
            "text": "As a matter of best practice we\u2019ll update our packages:  apt-get update  The build-essential package should already be installed, however, we\u2019re going still going to include it in our command for installation:  apt-get install build-essential libssl-dev  To install or update nvm, you can use the install script using cURL:  curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bash  if you have below problem or after you type  nvm ls-remote  command it result N/A: curl: (77) error setting certificate verify locations:\n  CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none  head to this  1 :  or Wget:  wget -qO- https://raw.githubusercontent.com/creationix/nvm/v0.31.0/install.sh | bash  Don't forget to restart your terminal  Then you use the following command to list available versions of nodejs  nvm ls-remote  To download, compile, and install the latest v5.0.x release of node, do this:  nvm install 5.0  And then in any new shell just use the installed version:  nvm use 5.0  Or you can just run it:  nvm run 5.0 --version  Or, you can run any arbitrary command in a subshell with the desired version of node:  nvm exec 4.2 node --version  You can also get the path to the executable to where it was installed:  nvm which 5.0  Node Version Manager      how to solve https problem    ICU plug-in Github    Installing the ICU plug-in", 
            "title": "Install lastest Elasticdump with NVM"
        }
    ]
}